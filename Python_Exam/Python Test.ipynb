{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57bda6d",
   "metadata": {},
   "source": [
    "# Python Technical Interview - AI Agent Developer Position\n",
    "\n",
    "## Instructions\n",
    "This notebook contains 10 questions designed to test your Python skills and ability to work with AI-generated code. Each question has:\n",
    "- **Problem Description** - What you need to accomplish\n",
    "- **Code Cell** - Where you write your solution\n",
    "- **Test Cell** - Automated tests to verify your solution\n",
    "\n",
    "**Guidelines:**\n",
    "- Read each question carefully\n",
    "- You can use whatever libraries or packages\n",
    "- Some questions provide starter code, others start from scratch\n",
    "- Focus on writing clean, readable, and robust code\n",
    "- code should be able to run after clearing all outputs\n",
    "- All test cells should pass when you're done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2500ce",
   "metadata": {},
   "source": [
    "## Question 1: Debug AI-Generated Code (Lists & Logic)\n",
    "\n",
    "**Scenario:** An AI generated this code to filter products by price range, but it has several bugs. Fix the code so it works correctly.\n",
    "\n",
    "**Requirements:**\n",
    "- Filter products where price is between min_price and max_price (inclusive)\n",
    "- Handle edge cases gracefully\n",
    "- Maintain the original function signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fab52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_products_by_price(products, min_price, max_price):\n",
    "    \"\"\"\n",
    "    Filter products by price range.\n",
    "    \n",
    "    Args:\n",
    "        products: List of dicts with 'name' and 'price' keys\n",
    "        min_price: Minimum price (inclusive)\n",
    "        max_price: Maximum price (inclusive)\n",
    "    \n",
    "    Returns:\n",
    "        List of products within price range\n",
    "    \"\"\"\n",
    "    # AI-generated buggy code below - FIX IT\n",
    "    filtered = []\n",
    "    for product in products:\n",
    "        if product['price'] > min_price and product['price'] < max_price:\n",
    "            filtered.append(product)\n",
    "    return filtered\n",
    "\n",
    "# Test your solution here\n",
    "products = [\n",
    "    {'name': 'Laptop', 'price': 1000},\n",
    "    {'name': 'Mouse', 'price': 25},\n",
    "    {'name': 'Keyboard', 'price': 75},\n",
    "    {'name': 'Monitor', 'price': 300}\n",
    "]\n",
    "\n",
    "result = filter_products_by_price(products, 25, 300)\n",
    "print(\"Filtered products:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "def test_question_1():\n",
    "    products = [\n",
    "        {'name': 'Laptop', 'price': 1000},\n",
    "        {'name': 'Mouse', 'price': 25},\n",
    "        {'name': 'Keyboard', 'price': 75},\n",
    "        {'name': 'Monitor', 'price': 300}\n",
    "    ]\n",
    "    \n",
    "    # Test inclusive bounds\n",
    "    result = filter_products_by_price(products, 25, 300)\n",
    "    expected_names = ['Mouse', 'Keyboard', 'Monitor']\n",
    "    actual_names = [p['name'] for p in result]\n",
    "    assert set(actual_names) == set(expected_names), f\"Expected {expected_names}, got {actual_names}\"\n",
    "    \n",
    "    # Test edge case - empty list\n",
    "    assert filter_products_by_price([], 0, 100) == []\n",
    "    \n",
    "    # Test no matches\n",
    "    assert filter_products_by_price(products, 2000, 3000) == []\n",
    "    \n",
    "    print(\"✓ Question 1 tests passed!\")\n",
    "\n",
    "test_question_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd1256e",
   "metadata": {},
   "source": [
    "## Question 2: Fix API Integration (Error Handling)\n",
    "\n",
    "**Scenario:** This AI-generated code fetches user data from an API but lacks proper error handling. Add robust error handling and improve the code.\n",
    "\n",
    "**Requirements:**\n",
    "- Handle network timeouts\n",
    "- Handle HTTP errors (4xx, 5xx)\n",
    "- Handle JSON parsing errors\n",
    "- Return None on any error, don't let exceptions bubble up\n",
    "- Add appropriate logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964dbc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_user_data(user_id):\n",
    "    \"\"\"\n",
    "    Fetch user data from API with proper error handling.\n",
    "    \n",
    "    Args:\n",
    "        user_id: User ID to fetch\n",
    "        \n",
    "    Returns:\n",
    "        dict: User data if successful, None if any error occurs\n",
    "    \"\"\"\n",
    "    # AI-generated code with poor error handling - IMPROVE IT\n",
    "    url = f\"https://jsonplaceholder.typicode.com/users/{user_id}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data\n",
    "\n",
    "# Test your solution here\n",
    "user_data = get_user_data(1)\n",
    "print(\"User data:\", user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "import unittest.mock as mock\n",
    "\n",
    "def test_question_2():\n",
    "    # Test successful request\n",
    "    user_data = get_user_data(1)\n",
    "    assert user_data is not None\n",
    "    assert 'name' in user_data\n",
    "    \n",
    "    # Test invalid user ID\n",
    "    user_data = get_user_data(999999)\n",
    "    assert user_data is None\n",
    "    \n",
    "    # Test with mock to simulate network error\n",
    "    with mock.patch('requests.get') as mock_get:\n",
    "        mock_get.side_effect = requests.exceptions.RequestException(\"Network error\")\n",
    "        result = get_user_data(1)\n",
    "        assert result is None\n",
    "    \n",
    "    # Test with mock to simulate timeout\n",
    "    with mock.patch('requests.get') as mock_get:\n",
    "        mock_get.side_effect = requests.exceptions.Timeout(\"Timeout\")\n",
    "        result = get_user_data(1)\n",
    "        assert result is None\n",
    "    \n",
    "    print(\"✓ Question 2 tests passed!\")\n",
    "\n",
    "test_question_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a7f14",
   "metadata": {},
   "source": [
    "## Question 3: Code from Scratch (Data Structures)\n",
    "\n",
    "**Scenario:** Create a `TaskManager` class to manage a simple todo list.\n",
    "\n",
    "**Requirements:**\n",
    "- Add tasks with priority (1=high, 2=medium, 3=low)\n",
    "- Mark tasks as complete\n",
    "- Get tasks filtered by completion status and/or priority\n",
    "- Get task count by status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskManager:\n",
    "    \"\"\"\n",
    "    A simple task manager for tracking todo items.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize empty task manager.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def add_task(self, description, priority=2):\n",
    "        \"\"\"\n",
    "        Add a new task.\n",
    "        \n",
    "        Args:\n",
    "            description (str): Task description\n",
    "            priority (int): Priority level (1=high, 2=medium, 3=low)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def complete_task(self, task_id):\n",
    "        \"\"\"\n",
    "        Mark a task as complete.\n",
    "        \n",
    "        Args:\n",
    "            task_id: Unique identifier for the task\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if task was found and completed, False otherwise\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_tasks(self, completed=None, priority=None):\n",
    "        \"\"\"\n",
    "        Get tasks filtered by status and/or priority.\n",
    "        \n",
    "        Args:\n",
    "            completed (bool, optional): Filter by completion status\n",
    "            priority (int, optional): Filter by priority level\n",
    "            \n",
    "        Returns:\n",
    "            list: List of matching tasks\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_task_count(self, completed=None):\n",
    "        \"\"\"\n",
    "        Get count of tasks by completion status.\n",
    "        \n",
    "        Args:\n",
    "            completed (bool, optional): Count completed (True) or pending (False) tasks\n",
    "            \n",
    "        Returns:\n",
    "            int: Number of matching tasks\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "tm = TaskManager()\n",
    "tm.add_task(\"Fix bug in login\", 1)  # High priority\n",
    "tm.add_task(\"Update documentation\", 3)  # Low priority\n",
    "tm.add_task(\"Code review\", 2)  # Medium priority\n",
    "\n",
    "print(\"All tasks:\", len(tm.get_tasks()))\n",
    "print(\"High priority tasks:\", len(tm.get_tasks(priority=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d50272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "def test_question_3():\n",
    "    tm = TaskManager()\n",
    "    \n",
    "    # Test adding tasks\n",
    "    tm.add_task(\"Task 1\", 1)\n",
    "    tm.add_task(\"Task 2\", 2)\n",
    "    tm.add_task(\"Task 3\", 3)\n",
    "    \n",
    "    # Test get all tasks\n",
    "    all_tasks = tm.get_tasks()\n",
    "    assert len(all_tasks) == 3\n",
    "    \n",
    "    # Test priority filtering\n",
    "    high_priority = tm.get_tasks(priority=1)\n",
    "    assert len(high_priority) == 1\n",
    "    \n",
    "    # Test task completion\n",
    "    task_id = all_tasks[0]['id']  # Assuming tasks have 'id' field\n",
    "    success = tm.complete_task(task_id)\n",
    "    assert success == True\n",
    "    \n",
    "    # Test completion filtering\n",
    "    completed_tasks = tm.get_tasks(completed=True)\n",
    "    assert len(completed_tasks) == 1\n",
    "    \n",
    "    pending_tasks = tm.get_tasks(completed=False)\n",
    "    assert len(pending_tasks) == 2\n",
    "    \n",
    "    # Test task counts\n",
    "    assert tm.get_task_count() == 3\n",
    "    assert tm.get_task_count(completed=True) == 1\n",
    "    assert tm.get_task_count(completed=False) == 2\n",
    "    \n",
    "    print(\"✓ Question 3 tests passed!\")\n",
    "\n",
    "test_question_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243a1dc",
   "metadata": {},
   "source": [
    "## Question 4: Optimize AI Code (Performance)\n",
    "\n",
    "**Scenario:** This AI code finds common elements between multiple lists, but it's very inefficient. Optimize it for better performance.\n",
    "\n",
    "**Requirements:**\n",
    "- Same functionality as original\n",
    "- Significantly better time complexity\n",
    "- Handle edge cases (empty lists, no common elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_elements_slow(lists):\n",
    "    \"\"\"\n",
    "    Find elements that appear in ALL provided lists.\n",
    "    AI-generated inefficient version - OPTIMIZE THIS!\n",
    "    \n",
    "    Args:\n",
    "        lists: List of lists to find common elements in\n",
    "        \n",
    "    Returns:\n",
    "        list: Elements that appear in all lists\n",
    "    \"\"\"\n",
    "    if not lists:\n",
    "        return []\n",
    "    \n",
    "    common = []\n",
    "    for item in lists[0]:\n",
    "        is_common = True\n",
    "        for other_list in lists[1:]:\n",
    "            found = False\n",
    "            for other_item in other_list:\n",
    "                if item == other_item:\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                is_common = False\n",
    "                break\n",
    "        if is_common and item not in common:\n",
    "            common.append(item)\n",
    "    \n",
    "    return common\n",
    "\n",
    "# Optimized version - implement this\n",
    "def find_common_elements_fast(lists):\n",
    "    \"\"\"\n",
    "    Find elements that appear in ALL provided lists.\n",
    "    Optimized version with better time complexity.\n",
    "    \n",
    "    Args:\n",
    "        lists: List of lists to find common elements in\n",
    "        \n",
    "    Returns:\n",
    "        list: Elements that appear in all lists\n",
    "    \"\"\"\n",
    "    # Your optimized implementation here\n",
    "    pass\n",
    "\n",
    "# Test both versions\n",
    "test_lists = [\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [3, 4, 5, 6, 7],\n",
    "    [4, 5, 7, 8, 9]\n",
    "]\n",
    "\n",
    "print(\"Slow version:\", find_common_elements_slow(test_lists))\n",
    "print(\"Fast version:\", find_common_elements_fast(test_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de081d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "\n",
    "import time\n",
    "\n",
    "def test_question_4():\n",
    "    # Basic functionality test\n",
    "    test_lists = [\n",
    "        [1, 2, 3, 4, 5],\n",
    "        [3, 4, 5, 6, 7],\n",
    "        [4, 5, 7, 8, 9]\n",
    "    ]\n",
    "    \n",
    "    slow_result = find_common_elements_slow(test_lists)\n",
    "    fast_result = find_common_elements_fast(test_lists)\n",
    "    \n",
    "    assert set(slow_result) == set(fast_result), \"Results don't match\"\n",
    "    assert set(fast_result) == {4, 5}, f\"Expected {{4, 5}}, got {set(fast_result)}\"\n",
    "    \n",
    "    # Edge cases\n",
    "    assert find_common_elements_fast([]) == []\n",
    "    assert find_common_elements_fast([[1, 2], []]) == []\n",
    "    assert find_common_elements_fast([[1, 2, 3]]) == [1, 2, 3]\n",
    "    \n",
    "    # Performance test (rough)\n",
    "    large_lists = [[i for i in range(1000)] for _ in range(10)]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    find_common_elements_fast(large_lists)\n",
    "    fast_time = time.time() - start_time\n",
    "    \n",
    "    # Fast version should complete in reasonable time\n",
    "    assert fast_time < 1.0, \"Optimized version is still too slow\"\n",
    "    \n",
    "    print(\"✓ Question 4 tests passed!\")\n",
    "\n",
    "test_question_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7131a24",
   "metadata": {},
   "source": [
    "## Question 5: Fix Function with Edge Cases\n",
    "\n",
    "**Scenario:** This AI function calculates statistics for a list of numbers, but fails on various edge cases. Make it robust.\n",
    "\n",
    "**Requirements:**\n",
    "- Handle empty lists\n",
    "- Handle non-numeric values gracefully\n",
    "- Handle division by zero\n",
    "- Return meaningful error messages or default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ffe4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(numbers):\n",
    "    \"\"\"\n",
    "    Calculate basic statistics for a list of numbers.\n",
    "    AI code that works for happy path but fails on edge cases - FIX IT!\n",
    "    \n",
    "    Args:\n",
    "        numbers: List of numbers\n",
    "        \n",
    "    Returns:\n",
    "        dict: Statistics including mean, median, mode, std_dev\n",
    "    \"\"\"\n",
    "    # Sort for median calculation\n",
    "    sorted_nums = sorted(numbers)\n",
    "    \n",
    "    # Mean\n",
    "    mean = sum(numbers) / len(numbers)\n",
    "    \n",
    "    # Median\n",
    "    n = len(numbers)\n",
    "    if n % 2 == 0:\n",
    "        median = (sorted_nums[n//2 - 1] + sorted_nums[n//2]) / 2\n",
    "    else:\n",
    "        median = sorted_nums[n//2]\n",
    "    \n",
    "    # Mode (most frequent)\n",
    "    from collections import Counter\n",
    "    counts = Counter(numbers)\n",
    "    mode = counts.most_common(1)[0][0]\n",
    "    \n",
    "    # Standard deviation\n",
    "    variance = sum((x - mean) ** 2 for x in numbers) / len(numbers)\n",
    "    std_dev = variance ** 0.5\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'median': median,\n",
    "        'mode': mode,\n",
    "        'std_dev': std_dev,\n",
    "        'count': len(numbers)\n",
    "    }\n",
    "\n",
    "# Test your solution\n",
    "test_cases = [\n",
    "    [1, 2, 3, 4, 5],           # Normal case\n",
    "    [],                        # Empty list\n",
    "    [1],                       # Single item\n",
    "    [1, 1, 1],                # All same\n",
    "    [1, 'invalid', 3],         # Mixed types\n",
    "    [1, 2, None, 4]           # None values\n",
    "]\n",
    "\n",
    "for i, case in enumerate(test_cases):\n",
    "    print(f\"Test case {i+1}: {case}\")\n",
    "    try:\n",
    "        result = calculate_stats(case)\n",
    "        print(f\"  Result: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b590186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "def test_question_5():\n",
    "    # Normal case\n",
    "    result = calculate_stats([1, 2, 3, 4, 5])\n",
    "    assert result['mean'] == 3.0\n",
    "    assert result['median'] == 3.0\n",
    "    assert result['count'] == 5\n",
    "    \n",
    "    # Single item\n",
    "    result = calculate_stats([42])\n",
    "    assert result['mean'] == 42\n",
    "    assert result['median'] == 42\n",
    "    assert result['mode'] == 42\n",
    "    assert result['std_dev'] == 0\n",
    "    \n",
    "    # Empty list - should handle gracefully\n",
    "    result = calculate_stats([])\n",
    "    assert 'error' in result or all(v is None or v == 0 for v in result.values())\n",
    "    \n",
    "    # Mixed types - should handle gracefully\n",
    "    result = calculate_stats([1, 'invalid', 3])\n",
    "    assert 'error' in result or result['count'] == 2  # Only valid numbers counted\n",
    "    \n",
    "    # All same values\n",
    "    result = calculate_stats([5, 5, 5, 5])\n",
    "    assert result['mean'] == 5\n",
    "    assert result['std_dev'] == 0\n",
    "    \n",
    "    print(\"✓ Question 5 tests passed!\")\n",
    "\n",
    "test_question_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c594b33",
   "metadata": {},
   "source": [
    "## Question 6: Complete Partial Implementation (Pandas/Data)\n",
    "\n",
    "### Goal\n",
    "Implement `analyze_sales_data(df, group_by_column)`.\n",
    "\n",
    "### Input\n",
    "A pandas DataFrame `df` with columns:\n",
    "- `product`\n",
    "- `category`\n",
    "- `sales`\n",
    "- `profit`\n",
    "\n",
    "### Output (must match exactly)\n",
    "- Return a DataFrame **indexed by `group_by_column`** (do not reset the index).\n",
    "- Include exactly these columns (names must match):\n",
    "  - `sales_sum` — sum of `sales`\n",
    "  - `sales_mean` — mean of `sales`\n",
    "  - `profit_sum` — sum of `profit`\n",
    "  - `profit_mean` — mean of `profit`\n",
    "  - `profit_margin` — `profit_sum / sales_sum` (use `NaN` if `sales_sum == 0`)\n",
    "- Handle missing values: treat missing `sales` or `profit` as `0` before aggregation.\n",
    "- Sorting is **not required**.\n",
    "\n",
    "### Edge Behavior\n",
    "- If `df` is empty or `group_by_column` is missing, return an empty DataFrame with the required column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_sales_data(df, group_by_column):\n",
    "    \"\"\"\n",
    "    Analyze sales data by grouping and calculating statistics.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['product', 'category', 'sales', 'profit']\n",
    "        group_by_column: Column name to group by\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with aggregated statistics\n",
    "    \"\"\"\n",
    "    # TODO: Implement the following logic:\n",
    "    # 1. Group by the specified column\n",
    "    # 2. Calculate sum, mean, and count for 'sales' and 'profit'\n",
    "    # 3. Calculate profit margin (profit/sales) for each group\n",
    "    # 4. Sort by total sales (descending)\n",
    "    # 5. Handle any missing values appropriately\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Create sample data for testing\n",
    "sample_data = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'C', 'A', 'B', 'C', 'A'],\n",
    "    'category': ['Electronics', 'Electronics', 'Clothing', 'Electronics', 'Electronics', 'Clothing', 'Electronics'],\n",
    "    'sales': [100, 200, 150, 120, np.nan, 180, 110],\n",
    "    'profit': [20, 50, 30, 25, 40, 35, 22]\n",
    "})\n",
    "\n",
    "print(\"Sample data:\")\n",
    "print(sample_data)\n",
    "print(\"\\nAnalysis by product:\")\n",
    "result = analyze_sales_data(sample_data, 'product')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a0c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "def test_question_6():\n",
    "    # Create test data\n",
    "    test_data = pd.DataFrame({\n",
    "        'product': ['A', 'B', 'A', 'B', 'A'],\n",
    "        'category': ['Cat1', 'Cat2', 'Cat1', 'Cat2', 'Cat1'],\n",
    "        'sales': [100, 200, 150, 300, 50],\n",
    "        'profit': [20, 40, 30, 60, 10]\n",
    "    })\n",
    "    \n",
    "    # Test grouping by product\n",
    "    result = analyze_sales_data(test_data, 'product')\n",
    "    \n",
    "    # Check structure\n",
    "    assert isinstance(result, pd.DataFrame), \"Should return DataFrame\"\n",
    "    assert len(result) == 2, \"Should have 2 groups (A and B)\"\n",
    "    \n",
    "    # Check required columns exist\n",
    "    required_cols = ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']\n",
    "    for col in required_cols:\n",
    "        assert col in result.columns, f\"Missing column: {col}\"\n",
    "    \n",
    "    # Check calculations for product A\n",
    "    product_a = result.loc['A'] if 'A' in result.index else result[result.index == 'A'].iloc[0]\n",
    "    assert product_a['sales_sum'] == 300, \"Product A sales sum should be 300\"\n",
    "    assert product_a['profit_sum'] == 60, \"Product A profit sum should be 60\"\n",
    "    \n",
    "    print(\"✓ Question 6 tests passed!\")\n",
    "\n",
    "test_question_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b18d21",
   "metadata": {},
   "source": [
    "## Question 7: Refactor Messy AI Code (Clean Code)\n",
    "\n",
    "**Scenario:** This AI code works but is poorly structured and hard to maintain. Refactor it following clean code principles.\n",
    "\n",
    "**Requirements:**\n",
    "- Improve readability and maintainability\n",
    "- Add proper documentation\n",
    "- Follow naming conventions\n",
    "- Break down large functions\n",
    "- Add type hints if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    \"\"\"Messy AI-generated code that works but needs refactoring - CLEAN IT UP!\"\"\"\n",
    "    result = {}\n",
    "    for item in data:\n",
    "        if 'type' in item and item['type'] == 'user':\n",
    "            if 'active' in item and item['active']:\n",
    "                if 'age' in item:\n",
    "                    if item['age'] >= 18:\n",
    "                        if 'email' in item and '@' in item['email']:\n",
    "                            category = 'adult'\n",
    "                            if item['age'] >= 65:\n",
    "                                category = 'senior'\n",
    "                            elif item['age'] >= 25:\n",
    "                                category = 'adult'\n",
    "                            else:\n",
    "                                category = 'young_adult'\n",
    "                            \n",
    "                            if category not in result:\n",
    "                                result[category] = {'count': 0, 'emails': [], 'total_age': 0}\n",
    "                            \n",
    "                            result[category]['count'] += 1\n",
    "                            result[category]['emails'].append(item['email'])\n",
    "                            result[category]['total_age'] += item['age']\n",
    "    \n",
    "    # Calculate averages\n",
    "    for cat in result:\n",
    "        result[cat]['avg_age'] = result[cat]['total_age'] / result[cat]['count']\n",
    "        del result[cat]['total_age']\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test data\n",
    "test_data = [\n",
    "    {'type': 'user', 'active': True, 'age': 25, 'email': 'user1@test.com'},\n",
    "    {'type': 'user', 'active': True, 'age': 70, 'email': 'user2@test.com'},\n",
    "    {'type': 'user', 'active': False, 'age': 30, 'email': 'user3@test.com'},\n",
    "    {'type': 'admin', 'active': True, 'age': 35, 'email': 'admin@test.com'},\n",
    "    {'type': 'user', 'active': True, 'age': 20, 'email': 'invalid-email'},\n",
    "    {'type': 'user', 'active': True, 'age': 40, 'email': 'user4@test.com'},\n",
    "]\n",
    "\n",
    "# Your refactored version should produce the same results\n",
    "original_result = process_data(test_data)\n",
    "print(\"Original result:\", original_result)\n",
    "\n",
    "# TODO: Create your clean, refactored version here\n",
    "def process_user_data_clean(data):\n",
    "    \"\"\"\n",
    "    Refactored version with clean code principles.\n",
    "    Add your implementation here.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Test both versions\n",
    "clean_result = process_user_data_clean(test_data)\n",
    "print(\"Clean result:\", clean_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "def test_question_7():\n",
    "    test_data = [\n",
    "        {'type': 'user', 'active': True, 'age': 25, 'email': 'user1@test.com'},\n",
    "        {'type': 'user', 'active': True, 'age': 70, 'email': 'user2@test.com'},\n",
    "        {'type': 'user', 'active': False, 'age': 30, 'email': 'user3@test.com'},\n",
    "        {'type': 'user', 'active': True, 'age': 20, 'email': 'user4@test.com'},\n",
    "    ]\n",
    "    \n",
    "    original_result = process_data(test_data)\n",
    "    clean_result = process_user_data_clean(test_data)\n",
    "    \n",
    "    # Results should be functionally equivalent\n",
    "    assert set(original_result.keys()) == set(clean_result.keys()), \"Categories don't match\"\n",
    "    \n",
    "    for category in original_result:\n",
    "        assert original_result[category]['count'] == clean_result[category]['count'], f\"Count mismatch for {category}\"\n",
    "        assert abs(original_result[category]['avg_age'] - clean_result[category]['avg_age']) < 0.01, f\"Average age mismatch for {category}\"\n",
    "    \n",
    "    print(\"✓ Question 7 tests passed!\")\n",
    "\n",
    "test_question_7()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf968be",
   "metadata": {},
   "source": [
    "## Question 8: Debug Complex Logic (Algorithms)\n",
    "\n",
    "**Scenario:** This AI implementation of binary search has subtle bugs. Find and fix all the issues.\n",
    "\n",
    "**Requirements:**\n",
    "- Fix the binary search algorithm\n",
    "- Handle edge cases properly\n",
    "- Maintain O(log n) time complexity\n",
    "- Return correct index or -1 if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b759b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search_buggy(arr, target):\n",
    "    \"\"\"\n",
    "    Binary search implementation with bugs - FIND AND FIX THEM!\n",
    "    \n",
    "    Args:\n",
    "        arr: Sorted list of integers\n",
    "        target: Value to search for\n",
    "        \n",
    "    Returns:\n",
    "        int: Index of target if found, -1 otherwise\n",
    "    \"\"\"\n",
    "    left = 0\n",
    "    right = len(arr)\n",
    "    \n",
    "    while left < right:\n",
    "        mid = (left + right) // 2\n",
    "        \n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid\n",
    "        else:\n",
    "            right = mid\n",
    "    \n",
    "    return -1\n",
    "\n",
    "# Test cases\n",
    "test_arrays = [\n",
    "    ([1, 3, 5, 7, 9, 11], 7),    # Should find at index 3\n",
    "    ([1, 3, 5, 7, 9, 11], 1),    # Should find at index 0\n",
    "    ([1, 3, 5, 7, 9, 11], 11),   # Should find at index 5\n",
    "    ([1, 3, 5, 7, 9, 11], 6),    # Should return -1\n",
    "    ([5], 5),                     # Single element found\n",
    "    ([5], 3),                     # Single element not found\n",
    "    ([], 5),                      # Empty array\n",
    "]\n",
    "\n",
    "for arr, target in test_arrays:\n",
    "    result = binary_search_buggy(arr, target)\n",
    "    print(f\"Searching for {target} in {arr}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e288e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "def test_question_8():\n",
    "    # Test cases with expected results\n",
    "    test_cases = [\n",
    "        ([1, 3, 5, 7, 9, 11], 7, 3),      # Found at index 3\n",
    "        ([1, 3, 5, 7, 9, 11], 1, 0),      # Found at index 0\n",
    "        ([1, 3, 5, 7, 9, 11], 11, 5),     # Found at index 5\n",
    "        ([1, 3, 5, 7, 9, 11], 6, -1),     # Not found\n",
    "        ([1, 3, 5, 7, 9, 11], 0, -1),     # Less than min\n",
    "        ([1, 3, 5, 7, 9, 11], 12, -1),    # Greater than max\n",
    "        ([5], 5, 0),                       # Single element found\n",
    "        ([5], 3, -1),                      # Single element not found\n",
    "        ([], 5, -1),                       # Empty array\n",
    "    ]\n",
    "    \n",
    "    for arr, target, expected in test_cases:\n",
    "        result = binary_search_buggy(arr, target)\n",
    "        assert result == expected, f\"Failed for {target} in {arr}: expected {expected}, got {result}\"\n",
    "    \n",
    "    # Test that it actually uses binary search (check performance)\n",
    "    large_array = list(range(0, 10000, 2))  # [0, 2, 4, 6, ..., 9998]\n",
    "    result = binary_search_buggy(large_array, 5000)\n",
    "    assert result == 2500, \"Should find 5000 at index 2500\"\n",
    "    \n",
    "    print(\"✓ Question 8 tests passed!\")\n",
    "\n",
    "test_question_8()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ecefa",
   "metadata": {},
   "source": [
    "## Question 9: Add Missing Functionality\n",
    "\n",
    "**Scenario:** This AI code provides a basic cache implementation but is missing several key features. Add the missing functionality to make it production-ready.\n",
    "\n",
    "**Requirements:**\n",
    "- Add TTL (time-to-live) support for automatic expiration\n",
    "- Add size limit with LRU (Least Recently Used) eviction\n",
    "- Add cache statistics tracking (hits, misses, evictions)\n",
    "- Add methods for cache management (clear, size, cleanup)\n",
    "- Handle thread safety considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cafdbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any, Optional, Dict\n",
    "from collections import OrderedDict\n",
    "\n",
    "class SimpleCache:\n",
    "    \"\"\"\n",
    "    AI-generated basic cache - ADD MISSING FEATURES!\n",
    "    Current features: basic get/set/delete\n",
    "    Missing: TTL, size limits, LRU eviction, statistics, management methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100, default_ttl: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize cache with size limit and default TTL.\n",
    "        \n",
    "        Args:\n",
    "            max_size: Maximum number of items to store\n",
    "            default_ttl: Default time-to-live in seconds (None = no expiration)\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.default_ttl = default_ttl\n",
    "        # Basic storage - you need to enhance this\n",
    "        self._data = {}\n",
    "        \n",
    "        # TODO: Add data structures for:\n",
    "        # - TTL tracking (when items expire)\n",
    "        # - LRU tracking (access order)\n",
    "        # - Statistics (hits, misses, evictions)\n",
    "    \n",
    "    def get(self, key: str) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Get value from cache.\n",
    "        \n",
    "        Args:\n",
    "            key: Cache key\n",
    "            \n",
    "        Returns:\n",
    "            Cached value or None if not found/expired\n",
    "        \"\"\"\n",
    "        # TODO: Implement the following:\n",
    "        # 1. Check if key exists\n",
    "        # 2. Check if item has expired (TTL)\n",
    "        # 3. Update LRU order (move to end)\n",
    "        # 4. Update hit/miss statistics\n",
    "        # 5. Clean up expired items\n",
    "        \n",
    "        return self._data.get(key)\n",
    "    \n",
    "    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Set value in cache.\n",
    "        \n",
    "        Args:\n",
    "            key: Cache key\n",
    "            value: Value to cache\n",
    "            ttl: Time-to-live in seconds (overrides default)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the following:\n",
    "        # 1. Calculate expiration time if TTL provided\n",
    "        # 2. Check if cache is full and evict LRU items\n",
    "        # 3. Store value with metadata\n",
    "        # 4. Update LRU order\n",
    "        # 5. Update statistics\n",
    "        \n",
    "        self._data[key] = value\n",
    "    \n",
    "    def delete(self, key: str) -> bool:\n",
    "        \"\"\"Delete key from cache.\"\"\"\n",
    "        if key in self._data:\n",
    "            del self._data[key]\n",
    "            # TODO: Also remove from TTL and LRU tracking\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    # TODO: Implement these missing methods:\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear all items from cache.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def size(self) -> int:\n",
    "        \"\"\"Return current number of items in cache.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Get cache statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with keys: hits, misses, evictions, current_size\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def cleanup_expired(self) -> int:\n",
    "        \"\"\"\n",
    "        Remove expired items from cache.\n",
    "        \n",
    "        Returns:\n",
    "            Number of items removed\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _evict_lru(self, count: int = 1) -> int:\n",
    "        \"\"\"\n",
    "        Evict least recently used items.\n",
    "        \n",
    "        Args:\n",
    "            count: Number of items to evict\n",
    "            \n",
    "        Returns:\n",
    "            Number of items actually evicted\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _is_expired(self, key: str) -> bool:\n",
    "        \"\"\"Check if a cache entry has expired.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test your enhanced implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Test TTL functionality\n",
    "    cache = SimpleCache(max_size=3, default_ttl=1)  # 1 second TTL\n",
    "    \n",
    "    print(\"=== Testing TTL ===\")\n",
    "    cache.set(\"temp_key\", \"temp_value\")\n",
    "    print(f\"Immediately after set: {cache.get('temp_key')}\")\n",
    "    time.sleep(1.1)\n",
    "    print(f\"After TTL expired: {cache.get('temp_key')}\")\n",
    "    \n",
    "    print(\"\\n=== Testing Size Limits & LRU ===\")\n",
    "    cache.clear()\n",
    "    cache.set(\"a\", 1, ttl=None)  # No expiration\n",
    "    cache.set(\"b\", 2, ttl=None)\n",
    "    cache.set(\"c\", 3, ttl=None)\n",
    "    print(f\"Cache size after adding 3 items: {cache.size()}\")\n",
    "    \n",
    "    # Access 'a' to make it recently used\n",
    "    cache.get(\"a\")\n",
    "    \n",
    "    # Add 'd' which should evict 'b' (least recently used)\n",
    "    cache.set(\"d\", 4, ttl=None)\n",
    "    print(f\"After adding 'd': a={cache.get('a')}, b={cache.get('b')}, c={cache.get('c')}, d={cache.get('d')}\")\n",
    "    \n",
    "    print(\"\\n=== Testing Statistics ===\")\n",
    "    stats = cache.get_stats()\n",
    "    print(f\"Cache statistics: {stats}\")\n",
    "    \n",
    "    print(\"\\n=== Testing Cleanup ===\")\n",
    "    cache.set(\"expire_me\", \"value\", ttl=1)\n",
    "    time.sleep(1.1)\n",
    "    removed_count = cache.cleanup_expired()\n",
    "    print(f\"Expired items removed: {removed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell \n",
    "import time\n",
    "\n",
    "def test_question_9():\n",
    "    print(\"Testing enhanced cache implementation...\")\n",
    "    \n",
    "    # Test 1: Basic functionality\n",
    "    cache = SimpleCache(max_size=3, default_ttl=60)\n",
    "    \n",
    "    cache.set(\"key1\", \"value1\")\n",
    "    cache.set(\"key2\", \"value2\")\n",
    "    \n",
    "    assert cache.get(\"key1\") == \"value1\", \"Basic get/set failed\"\n",
    "    assert cache.get(\"key2\") == \"value2\", \"Basic get/set failed\"\n",
    "    assert cache.size() == 2, f\"Expected size 2, got {cache.size()}\"\n",
    "    \n",
    "    # Test 2: TTL expiration\n",
    "    cache.clear()\n",
    "    cache.set(\"ttl_key\", \"ttl_value\", ttl=1)  # 1 second TTL\n",
    "    assert cache.get(\"ttl_key\") == \"ttl_value\", \"TTL key should be accessible immediately\"\n",
    "    \n",
    "    time.sleep(1.1)  # Wait for expiration\n",
    "    assert cache.get(\"ttl_key\") is None, \"TTL key should be expired and return None\"\n",
    "    \n",
    "    # Test 3: Size limits and LRU eviction\n",
    "    cache.clear()\n",
    "    cache.set(\"a\", 1)\n",
    "    cache.set(\"b\", 2) \n",
    "    cache.set(\"c\", 3)  # Cache is now full (max_size=3)\n",
    "    \n",
    "    # Access 'a' to make it recently used\n",
    "    cache.get(\"a\")\n",
    "    \n",
    "    # Add 'd' which should evict 'b' (least recently used)\n",
    "    cache.set(\"d\", 4)\n",
    "    \n",
    "    assert cache.get(\"a\") == 1, \"Recently used 'a' should not be evicted\"\n",
    "    assert cache.get(\"b\") is None, \"Least recently used 'b' should be evicted\"\n",
    "    assert cache.get(\"c\") == 3, \"'c' should still be in cache\"\n",
    "    assert cache.get(\"d\") == 4, \"Newly added 'd' should be in cache\"\n",
    "    assert cache.size() == 3, \"Cache size should remain at max_size\"\n",
    "    \n",
    "    # Test 4: Statistics tracking\n",
    "    cache.clear()\n",
    "    cache.set(\"stat_key\", \"stat_value\")\n",
    "    cache.get(\"stat_key\")  # Hit\n",
    "    cache.get(\"nonexistent\")  # Miss\n",
    "    \n",
    "    stats = cache.get_stats()\n",
    "    required_stats = [\"hits\", \"misses\", \"evictions\", \"current_size\"]\n",
    "    for stat in required_stats:\n",
    "        assert stat in stats, f\"Missing statistic: {stat}\"\n",
    "    \n",
    "    assert stats[\"hits\"] > 0, \"Should have recorded hits\"\n",
    "    assert stats[\"misses\"] > 0, \"Should have recorded misses\"\n",
    "    assert stats[\"current_size\"] == 1, \"Should track current size\"\n",
    "    \n",
    "    # Test 5: Manual cleanup\n",
    "    cache.clear()\n",
    "    cache.set(\"expire1\", \"value1\", ttl=1)\n",
    "    cache.set(\"expire2\", \"value2\", ttl=1)\n",
    "    cache.set(\"keep\", \"value3\", ttl=None)  # No expiration\n",
    "    \n",
    "    time.sleep(1.1)  # Wait for expiration\n",
    "    removed_count = cache.cleanup_expired()\n",
    "    \n",
    "    assert removed_count == 2, f\"Should have removed 2 expired items, removed {removed_count}\"\n",
    "    assert cache.get(\"keep\") == \"value3\", \"Non-expiring item should remain\"\n",
    "    assert cache.size() == 1, \"Only one item should remain after cleanup\"\n",
    "    \n",
    "    # Test 6: Edge cases\n",
    "    cache.clear()\n",
    "    assert cache.size() == 0, \"Cache should be empty after clear\"\n",
    "    assert cache.get(\"nonexistent\") is None, \"Getting non-existent key should return None\"\n",
    "    assert cache.delete(\"nonexistent\") == False, \"Deleting non-existent key should return False\"\n",
    "    \n",
    "    # Test delete functionality\n",
    "    cache.set(\"delete_me\", \"value\")\n",
    "    assert cache.delete(\"delete_me\") == True, \"Deleting existing key should return True\"\n",
    "    assert cache.get(\"delete_me\") is None, \"Deleted key should not be accessible\"\n",
    "    \n",
    "    print(\"✓ All Question 9 tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391df2c",
   "metadata": {},
   "source": [
    "## Question 10: Integration Challenge (Multiple Components)\n",
    "\n",
    "**Scenario:** You have three separate AI-generated modules that need to work together in a data processing pipeline, but they have interface mismatches and compatibility issues. Your job is to create the integration layer that makes them work together seamlessly.\n",
    "\n",
    "**Requirements:**\n",
    "- Create adapter/wrapper functions to handle data format conversions\n",
    "- Build a unified pipeline that chains all three components\n",
    "- Add comprehensive error handling for the integration\n",
    "- Handle edge cases and invalid data gracefully\n",
    "- Create helper functions for data transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a060da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "\n",
    "# Component 1: Data Processor (returns dict with specific structure)\n",
    "class DataProcessor:\n",
    "    \"\"\"AI Component 1 - processes raw data and returns structured dict\"\"\"\n",
    "    \n",
    "    def process_data(self, raw_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Process raw data and return structured dict.\"\"\"\n",
    "        if not isinstance(raw_data, list):\n",
    "            raise ValueError(\"Expected list input\")\n",
    "        \n",
    "        result = {\n",
    "            'total_items': len(raw_data),\n",
    "            'processed_items': [],\n",
    "            'metadata': {'processing_time': 0.1, 'timestamp': '2024-01-01T12:00:00Z'}\n",
    "        }\n",
    "        \n",
    "        for item in raw_data:\n",
    "            if isinstance(item, dict) and 'value' in item:\n",
    "                result['processed_items'].append({\n",
    "                    'id': item.get('id', 'unknown'),\n",
    "                    'processed_value': item['value'] * 2,\n",
    "                    'original_value': item['value'],\n",
    "                    'status': 'processed'\n",
    "                })\n",
    "            else:\n",
    "                result['processed_items'].append({\n",
    "                    'id': 'error',\n",
    "                    'processed_value': 0,\n",
    "                    'original_value': None,\n",
    "                    'status': 'failed'\n",
    "                })\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Component 2: Analytics Engine (expects JSON string, returns tuple)\n",
    "class AnalyticsEngine:\n",
    "    \"\"\"AI Component 2 - performs analytics on data, expects JSON string input\"\"\"\n",
    "    \n",
    "    def analyze(self, json_data_string: str) -> Tuple[Optional[str], Union[Dict[str, float], str]]:\n",
    "        \"\"\"Analyze data from JSON string, return (summary, metrics) tuple.\"\"\"\n",
    "        try:\n",
    "            data = json.loads(json_data_string)\n",
    "        except json.JSONDecodeError:\n",
    "            return None, \"Invalid JSON format\"\n",
    "        \n",
    "        if not isinstance(data, dict) or 'processed_items' not in data:\n",
    "            return None, \"Missing processed_items in data structure\"\n",
    "        \n",
    "        items = data['processed_items']\n",
    "        if not isinstance(items, list):\n",
    "            return None, \"processed_items must be a list\"\n",
    "        \n",
    "        # Extract numeric values for analysis\n",
    "        values = []\n",
    "        failed_count = 0\n",
    "        \n",
    "        for item in items:\n",
    "            if isinstance(item, dict) and item.get('status') == 'processed':\n",
    "                if 'processed_value' in item and isinstance(item['processed_value'], (int, float)):\n",
    "                    values.append(item['processed_value'])\n",
    "            else:\n",
    "                failed_count += 1\n",
    "        \n",
    "        if not values:\n",
    "            return None, \"No valid numeric data found for analysis\"\n",
    "        \n",
    "        summary = f\"Analyzed {len(items)} items ({len(values)} successful, {failed_count} failed)\"\n",
    "        metrics = {\n",
    "            'avg_value': sum(values) / len(values),\n",
    "            'max_value': max(values),\n",
    "            'min_value': min(values),\n",
    "            'total_value': sum(values),\n",
    "            'success_rate': len(values) / len(items) if items else 0.0\n",
    "        }\n",
    "        \n",
    "        return summary, metrics\n",
    "\n",
    "# Component 3: Report Generator (expects list of tuples, returns formatted string)\n",
    "class ReportGenerator:\n",
    "    \"\"\"AI Component 3 - generates reports from analytics results\"\"\"\n",
    "    \n",
    "    def generate_report(self, analytics_results_list: List[Tuple[Optional[str], Union[Dict, str]]]) -> str:\n",
    "        \"\"\"Generate report from list of (summary, metrics) tuples.\"\"\"\n",
    "        if not isinstance(analytics_results_list, list):\n",
    "            return \"Error: Expected list input for report generation\"\n",
    "        \n",
    "        if not analytics_results_list:\n",
    "            return \"Error: No data provided for report generation\"\n",
    "        \n",
    "        report_lines = [\n",
    "            \"=\" * 50,\n",
    "            \"           ANALYSIS REPORT\",\n",
    "            \"=\" * 50\n",
    "        ]\n",
    "        \n",
    "        for i, result in enumerate(analytics_results_list):\n",
    "            if not isinstance(result, tuple) or len(result) != 2:\n",
    "                report_lines.append(f\"\\nSection {i+1}: Invalid data format - expected (summary, metrics) tuple\")\n",
    "                continue\n",
    "            \n",
    "            summary, metrics = result\n",
    "            \n",
    "            if summary is None:\n",
    "                report_lines.append(f\"\\nSection {i+1}: Analysis failed\")\n",
    "                report_lines.append(f\"  Error: {metrics}\")\n",
    "                continue\n",
    "            \n",
    "            report_lines.append(f\"\\nSection {i+1}: {summary}\")\n",
    "            \n",
    "            if isinstance(metrics, dict):\n",
    "                report_lines.append(\"  Metrics:\")\n",
    "                for key, value in metrics.items():\n",
    "                    if isinstance(value, float):\n",
    "                        report_lines.append(f\"    {key}: {value:.2f}\")\n",
    "                    else:\n",
    "                        report_lines.append(f\"    {key}: {value}\")\n",
    "            else:\n",
    "                report_lines.append(f\"  Metrics: {metrics}\")\n",
    "        \n",
    "        report_lines.append(\"\\n\" + \"=\" * 50)\n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "# TODO: Implement the integration functions below\n",
    "\n",
    "def dict_to_json_adapter(data_dict: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Convert dictionary to JSON string for AnalyticsEngine.\n",
    "    \n",
    "    Args:\n",
    "        data_dict: Dictionary from DataProcessor\n",
    "        \n",
    "    Returns:\n",
    "        JSON string suitable for AnalyticsEngine\n",
    "    \"\"\"\n",
    "    # TODO: Implement JSON conversion with error handling\n",
    "    pass\n",
    "\n",
    "def validate_and_clean_raw_data(raw_data: Any) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Validate and clean raw input data.\n",
    "    \n",
    "    Args:\n",
    "        raw_data: Input data of any type\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned list of dictionaries\n",
    "    \"\"\"\n",
    "    # TODO: Implement data validation and cleaning\n",
    "    pass\n",
    "\n",
    "def integrated_pipeline(raw_data_list: List[Any]) -> str:\n",
    "    \"\"\"\n",
    "    Integrate all three components to process data end-to-end.\n",
    "    \n",
    "    This function should:\n",
    "    1. Validate and clean each raw dataset\n",
    "    2. Process each dataset through DataProcessor\n",
    "    3. Convert results to format expected by AnalyticsEngine\n",
    "    4. Run analytics on each processed dataset\n",
    "    5. Collect all analytics results\n",
    "    6. Generate final report using ReportGenerator\n",
    "    7. Handle all errors gracefully\n",
    "    \n",
    "    Args:\n",
    "        raw_data_list: List of raw data sets to process\n",
    "        \n",
    "    Returns:\n",
    "        str: Final report combining all analyses\n",
    "    \"\"\"\n",
    "    # TODO: Implement the complete integration pipeline\n",
    "    # Consider these steps:\n",
    "    \n",
    "    # Step 1: Initialize components\n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    \n",
    "    # Step 2: Process each dataset\n",
    "    analytics_results = []\n",
    "    \n",
    "    # Step 3: For each raw_data in raw_data_list:\n",
    "    #   - Validate and clean the data\n",
    "    #   - Process through DataProcessor\n",
    "    #   - Convert to JSON for AnalyticsEngine\n",
    "    #   - Run analytics\n",
    "    #   - Collect results\n",
    "    \n",
    "    # Step 4: Generate final report\n",
    "    # return reporter.generate_report(analytics_results)\n",
    "    \n",
    "    pass\n",
    "\n",
    "def create_sample_data() -> List[List[Dict[str, Any]]]:\n",
    "    \"\"\"Create sample test data for the pipeline.\"\"\"\n",
    "    return [\n",
    "        # Dataset 1: Normal data\n",
    "        [\n",
    "            {'id': 'A1', 'value': 10},\n",
    "            {'id': 'A2', 'value': 20},\n",
    "            {'id': 'A3', 'value': 15}\n",
    "        ],\n",
    "        # Dataset 2: Smaller dataset\n",
    "        [\n",
    "            {'id': 'B1', 'value': 5},\n",
    "            {'id': 'B2', 'value': 25}\n",
    "        ],\n",
    "        # Dataset 3: Mixed data with issues\n",
    "        [\n",
    "            {'id': 'C1', 'value': 30},\n",
    "            {'id': 'C2'},  # Missing value\n",
    "            {'value': 40},  # Missing id\n",
    "            {'id': 'C4', 'value': 'invalid'},  # Invalid value type\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "# Test the integration\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing component integration...\")\n",
    "    \n",
    "    # Test individual components first\n",
    "    print(\"\\n=== Testing Individual Components ===\")\n",
    "    \n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    \n",
    "    # Test DataProcessor\n",
    "    test_data = [{'id': 'test', 'value': 10}]\n",
    "    processed = processor.process_data(test_data)\n",
    "    print(f\"DataProcessor output: {processed}\")\n",
    "    \n",
    "    # Test AnalyticsEngine\n",
    "    json_data = json.dumps(processed)\n",
    "    analysis_result = analytics.analyze(json_data)\n",
    "    print(f\"AnalyticsEngine output: {analysis_result}\")\n",
    "    \n",
    "    # Test ReportGenerator\n",
    "    report = reporter.generate_report([analysis_result])\n",
    "    print(f\"ReportGenerator output:\\n{report}\")\n",
    "    \n",
    "    print(\"\\n=== Testing Integrated Pipeline ===\")\n",
    "    \n",
    "    # Test full pipeline\n",
    "    sample_datasets = create_sample_data()\n",
    "    \n",
    "    try:\n",
    "        final_report = integrated_pipeline(sample_datasets)\n",
    "        print(\"Integration successful!\")\n",
    "        print(final_report)\n",
    "    except Exception as e:\n",
    "        print(f\"Integration failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "def test_question_10():\n",
    "    print(\"Testing integrated pipeline...\")\n",
    "    \n",
    "    # Test 1: Individual component functionality\n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    \n",
    "    # Test DataProcessor\n",
    "    test_data = [{'id': 'test1', 'value': 10}, {'id': 'test2', 'value': 20}]\n",
    "    processed = processor.process_data(test_data)\n",
    "    \n",
    "    assert isinstance(processed, dict), \"DataProcessor should return dict\"\n",
    "    assert 'total_items' in processed, \"Missing total_items in processed data\"\n",
    "    assert 'processed_items' in processed, \"Missing processed_items in processed data\"\n",
    "    assert processed['total_items'] == 2, \"Should count items correctly\"\n",
    "    \n",
    "    # Test AnalyticsEngine\n",
    "    json_data = json.dumps(processed)\n",
    "    summary, metrics = analytics.analyze(json_data)\n",
    "    \n",
    "    assert summary is not None, \"Analytics should return valid summary\"\n",
    "    assert isinstance(metrics, dict), \"Analytics should return metrics dict\"\n",
    "    assert 'avg_value' in metrics, \"Missing avg_value in metrics\"\n",
    "    \n",
    "    # Test ReportGenerator\n",
    "    report = reporter.generate_report([(summary, metrics)])\n",
    "    \n",
    "    assert isinstance(report, str), \"Report should be string\"\n",
    "    assert \"ANALYSIS REPORT\" in report, \"Report should contain header\"\n",
    "    assert \"Section 1\" in report, \"Report should contain section\"\n",
    "    \n",
    "    # Test 2: Data validation and cleaning\n",
    "    cleaned_data = validate_and_clean_raw_data([\n",
    "        {'id': 'valid', 'value': 10},\n",
    "        {'value': 20},  # Missing id\n",
    "        {'id': 'invalid'},  # Missing value\n",
    "        'invalid_format'  # Wrong format\n",
    "    ])\n",
    "    \n",
    "    assert isinstance(cleaned_data, list), \"Should return list\"\n",
    "    # Should handle invalid data gracefully\n",
    "    \n",
    "    # Test 3: Integration adapters\n",
    "    test_dict = {'processed_items': [{'processed_value': 10}]}\n",
    "    json_str = dict_to_json_adapter(test_dict)\n",
    "    \n",
    "    assert isinstance(json_str, str), \"Should return JSON string\"\n",
    "    # Should be valid JSON\n",
    "    parsed = json.loads(json_str)\n",
    "    assert parsed == test_dict, \"Should preserve data structure\"\n",
    "    \n",
    "    # Test 4: Full pipeline integration\n",
    "    sample_datasets = [\n",
    "        [{'id': 'A1', 'value': 10}, {'id': 'A2', 'value': 20}],\n",
    "        [{'id': 'B1', 'value': 5}],\n",
    "        []  # Empty dataset\n",
    "    ]\n",
    "    \n",
    "    final_report = integrated_pipeline(sample_datasets)\n",
    "    \n",
    "    assert isinstance(final_report, str), \"Pipeline should return string report\"\n",
    "    assert \"ANALYSIS REPORT\" in final_report, \"Should contain report header\"\n",
    "    \n",
    "    # Should handle multiple sections\n",
    "    assert \"Section 1\" in final_report, \"Should have first section\"\n",
    "    assert \"Section 2\" in final_report, \"Should have second section\"\n",
    "    \n",
    "    # Test 5: Error handling\n",
    "    # Test with invalid input\n",
    "    error_report = integrated_pipeline([])\n",
    "    assert isinstance(error_report, str), \"Should handle empty input gracefully\"\n",
    "    \n",
    "    # Test with malformed data\n",
    "    malformed_report = integrated_pipeline([[\"not\", \"a\", \"dict\", \"list\"]])\n",
    "    assert isinstance(malformed_report, str), \"Should handle malformed data\"\n",
    "    \n",
    "    # Test 6: Edge cases\n",
    "    edge_cases = [\n",
    "        [{'id': 'only_id'}],  # Missing value\n",
    "        [{'value': 42}],      # Missing id\n",
    "        [{}],                 # Empty dict\n",
    "    ]\n",
    "    \n",
    "    edge_report = integrated_pipeline(edge_cases)\n",
    "    assert isinstance(edge_report, str), \"Should handle edge cases\"\n",
    "    assert \"ANALYSIS REPORT\" in edge_report, \"Should still generate report structure\"\n",
    "    \n",
    "    print(\"✓ All Question 10 tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_question_10()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4fe67",
   "metadata": {},
   "source": [
    "## Final Submission Instructions\n",
    "\n",
    "### Before You Submit:\n",
    "\n",
    "**Code Quality Checklist:**\n",
    "- All test cells pass without errors\n",
    "- Code follows Python best practices and conventions  \n",
    "- Functions include appropriate documentation\n",
    "- Error handling is implemented where required\n",
    "- Edge cases are handled appropriately\n",
    "- Code is clean, readable, and maintainable\n",
    "\n",
    "**Save Your Work:**\n",
    "- **Save all code outputs** - Run all cells and keep the output visible\n",
    "- Save the notebook file (Ctrl+S / Cmd+S)\n",
    "- Verify all your implementations are in the correct code cells\n",
    "- Double-check that test cells show \"tests passed!\" messages\n",
    "\n",
    "### Submission Format:\n",
    "Submit your completed `firstname_lastname.ipynb` file with **all outputs preserved**. We want to see:\n",
    "- Your code implementations\n",
    "- Test results (passed/failed)\n",
    "- Any debugging output or print statements\n",
    "- Cell execution numbers\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
