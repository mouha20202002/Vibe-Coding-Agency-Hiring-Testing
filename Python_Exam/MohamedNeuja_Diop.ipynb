{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4ac431-6e21-42de-b796-7c46d6e1835d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ***Question 1: Debug AI-Generated Code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a23829ed-ed0d-43c9-9023-6e11f8339268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_products_by_price(products, min_price, max_price):\n",
    "    \"\"\"\n",
    "    Filter products by price range.\n",
    "    \n",
    "    Args:\n",
    "        products: List of dicts with 'name' and 'price' keys\n",
    "        min_price: Minimum price (inclusive)\n",
    "        max_price: Maximum price (inclusive)\n",
    "    \n",
    "    Returns:\n",
    "        List of products within price range\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for product in products:\n",
    "        price = product.get('price')\n",
    "        if price is not None and min_price <= price <= max_price:\n",
    "            filtered.append(product)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad7009a1-0534-41af-ae80-1cd2b0d92296",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 1 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "def test_question_1():\n",
    "    products = [\n",
    "        {'name': 'Laptop', 'price': 1000},\n",
    "        {'name': 'Mouse', 'price': 25},\n",
    "        {'name': 'Keyboard', 'price': 75},\n",
    "        {'name': 'Monitor', 'price': 300}\n",
    "    ]\n",
    "    \n",
    "    # Test inclusive bounds\n",
    "    result = filter_products_by_price(products, 25, 300)\n",
    "    expected_names = ['Mouse', 'Keyboard', 'Monitor']\n",
    "    actual_names = [p['name'] for p in result]\n",
    "    assert set(actual_names) == set(expected_names), f\"Expected {expected_names}, got {actual_names}\"\n",
    "    \n",
    "    # Test edge case - empty list\n",
    "    assert filter_products_by_price([], 0, 100) == []\n",
    "    \n",
    "    # Test no matches\n",
    "    assert filter_products_by_price(products, 2000, 3000) == []\n",
    "    \n",
    "    print(\"✓ Question 1 tests passed!\")\n",
    "\n",
    "test_question_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938cd8c3-a405-4a15-8db6-802f12049ee0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Errors:\n",
    "\n",
    "Exclusive bounds: > and < exclude values equal to min_price or max_price. The instructions required the bounds to be inclusive.\n",
    "\n",
    "No key check: product['price'] raises an error if a product does not have a price key.\n",
    "\n",
    "No type handling: if price is not a number, the code crashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e570ebd-e8e4-41c9-8cdc-c60fe475b42d",
   "metadata": {},
   "source": [
    "# ***Question 2: Fix API Integration (Error handling)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "788a312d-3832-4afd-9413-9fe6a1eef220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_user_data(user_id):\n",
    "    \"\"\"\n",
    "    Fetch user data from API with proper error handling.\n",
    "    \n",
    "    Args:\n",
    "        user_id: User ID to fetch\n",
    "        \n",
    "    Returns:\n",
    "        dict: User data if successful, None if any error occurs\n",
    "    \"\"\"\n",
    "    url = f\"https://jsonplaceholder.typicode.com/users/{user_id}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad status\n",
    "        data = response.json()\n",
    "        return data\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Timeout occurred while fetching user {user_id}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"HTTP error for user {user_id}: {e}\")\n",
    "    except ValueError:\n",
    "        logger.error(f\"JSON parsing error for user {user_id}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66aaaa11-2794-43c2-bee6-de28db2282cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:HTTP error for user 999999: 404 Client Error: Not Found for url: https://jsonplaceholder.typicode.com/users/999999\n",
      "ERROR:__main__:HTTP error for user 1: Network error\n",
      "ERROR:__main__:Timeout occurred while fetching user 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 2 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "import unittest.mock as mock\n",
    "\n",
    "def test_question_2():\n",
    "    # Test successful request\n",
    "    user_data = get_user_data(1)\n",
    "    assert user_data is not None\n",
    "    assert 'name' in user_data\n",
    "    \n",
    "    # Test invalid user ID\n",
    "    user_data = get_user_data(999999)\n",
    "    assert user_data is None\n",
    "    \n",
    "    # Test with mock to simulate network error\n",
    "    with mock.patch('requests.get') as mock_get:\n",
    "        mock_get.side_effect = requests.exceptions.RequestException(\"Network error\")\n",
    "        result = get_user_data(1)\n",
    "        assert result is None\n",
    "    \n",
    "    # Test with mock to simulate timeout\n",
    "    with mock.patch('requests.get') as mock_get:\n",
    "        mock_get.side_effect = requests.exceptions.Timeout(\"Timeout\")\n",
    "        result = get_user_data(1)\n",
    "        assert result is None\n",
    "    \n",
    "    print(\"✓ Question 2 tests passed!\")\n",
    "\n",
    "test_question_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc74c66-e990-455c-bfe2-156d23400cbc",
   "metadata": {},
   "source": [
    "# ***Question 3: TaskManager Class***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f383e10d-8ad8-421b-8adb-ec934c4ef852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TaskManager:\n",
    "    \"\"\"\n",
    "    A simple task manager for tracking todo items.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        self.next_id = 1\n",
    "    \n",
    "    def add_task(self, description, priority=2):\n",
    "        task = {\n",
    "            'id': self.next_id,\n",
    "            'description': description,\n",
    "            'priority': priority,\n",
    "            'completed': False\n",
    "        }\n",
    "        self.tasks.append(task)\n",
    "        self.next_id += 1\n",
    "    \n",
    "    def complete_task(self, task_id):\n",
    "        for task in self.tasks:\n",
    "            if task['id'] == task_id:\n",
    "                task['completed'] = True\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_tasks(self, completed=None, priority=None):\n",
    "        result = self.tasks\n",
    "        if completed is not None:\n",
    "            result = [t for t in result if t['completed'] == completed]\n",
    "        if priority is not None:\n",
    "            result = [t for t in result if t['priority'] == priority]\n",
    "        return result\n",
    "    \n",
    "    def get_task_count(self, completed=None):\n",
    "        return len(self.get_tasks(completed=completed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a75d2c8e-4ee2-49c6-b45c-48aa879207c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 3 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "def test_question_3():\n",
    "    tm = TaskManager()\n",
    "    \n",
    "    # Test adding tasks\n",
    "    tm.add_task(\"Task 1\", 1)\n",
    "    tm.add_task(\"Task 2\", 2)\n",
    "    tm.add_task(\"Task 3\", 3)\n",
    "    \n",
    "    # Test get all tasks\n",
    "    all_tasks = tm.get_tasks()\n",
    "    assert len(all_tasks) == 3\n",
    "    \n",
    "    # Test priority filtering\n",
    "    high_priority = tm.get_tasks(priority=1)\n",
    "    assert len(high_priority) == 1\n",
    "    \n",
    "    # Test task completion\n",
    "    task_id = all_tasks[0]['id']  # Assuming tasks have 'id' field\n",
    "    success = tm.complete_task(task_id)\n",
    "    assert success == True\n",
    "    \n",
    "    # Test completion filtering\n",
    "    completed_tasks = tm.get_tasks(completed=True)\n",
    "    assert len(completed_tasks) == 1\n",
    "    \n",
    "    pending_tasks = tm.get_tasks(completed=False)\n",
    "    assert len(pending_tasks) == 2\n",
    "    \n",
    "    # Test task counts\n",
    "    assert tm.get_task_count() == 3\n",
    "    assert tm.get_task_count(completed=True) == 1\n",
    "    assert tm.get_task_count(completed=False) == 2\n",
    "    \n",
    "    print(\"✓ Question 3 tests passed!\")\n",
    "\n",
    "test_question_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a8c11-1111-4bb9-a285-615b0f8255fa",
   "metadata": {},
   "source": [
    "# ***Question 4: Optimize AI Code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1f82a87-47ab-45a0-a5e6-b70dc8f5135d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slow version: [4, 5]\n",
      "Fast version: [4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Slow version (for comparison)\n",
    "def find_common_elements_slow(lists):\n",
    "    \"\"\"\n",
    "    Find elements that appear in ALL provided lists.\n",
    "    AI-generated inefficient version - OPTIMIZE THIS!\n",
    "    \"\"\"\n",
    "    if not lists:\n",
    "        return []\n",
    "    \n",
    "    common = []\n",
    "    for item in lists[0]:\n",
    "        is_common = True\n",
    "        for other_list in lists[1:]:\n",
    "            found = False\n",
    "            for other_item in other_list:\n",
    "                if item == other_item:\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                is_common = False\n",
    "                break\n",
    "        if is_common and item not in common:\n",
    "            common.append(item)\n",
    "    return common\n",
    "\n",
    "# Optimized version\n",
    "def find_common_elements_fast(lists):\n",
    "    \"\"\"\n",
    "    Find elements that appear in ALL provided lists.\n",
    "    Optimized version using sets.\n",
    "    \"\"\"\n",
    "    if not lists:\n",
    "        return []\n",
    "    \n",
    "    common_set = set(lists[0])\n",
    "    for lst in lists[1:]:\n",
    "        common_set &= set(lst)\n",
    "        if not common_set:\n",
    "            return []\n",
    "    \n",
    "    return list(common_set)\n",
    "\n",
    "# Test both versions\n",
    "test_lists = [\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [3, 4, 5, 6, 7],\n",
    "    [4, 5, 7, 8, 9]\n",
    "]\n",
    "\n",
    "print(\"Slow version:\", find_common_elements_slow(test_lists))\n",
    "print(\"Fast version:\", find_common_elements_fast(test_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcc92970-529f-407f-8fa7-41a91e0fbd0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 4 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "\n",
    "import time\n",
    "\n",
    "def test_question_4():\n",
    "    # Basic functionality test\n",
    "    test_lists = [\n",
    "        [1, 2, 3, 4, 5],\n",
    "        [3, 4, 5, 6, 7],\n",
    "        [4, 5, 7, 8, 9]\n",
    "    ]\n",
    "    \n",
    "    slow_result = find_common_elements_slow(test_lists)\n",
    "    fast_result = find_common_elements_fast(test_lists)\n",
    "    \n",
    "    assert set(slow_result) == set(fast_result), \"Results don't match\"\n",
    "    assert set(fast_result) == {4, 5}, f\"Expected {{4, 5}}, got {set(fast_result)}\"\n",
    "    \n",
    "    # Edge cases\n",
    "    assert find_common_elements_fast([]) == []\n",
    "    assert find_common_elements_fast([[1, 2], []]) == []\n",
    "    assert find_common_elements_fast([[1, 2, 3]]) == [1, 2, 3]\n",
    "    \n",
    "    # Performance test (rough)\n",
    "    large_lists = [[i for i in range(1000)] for _ in range(10)]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    find_common_elements_fast(large_lists)\n",
    "    fast_time = time.time() - start_time\n",
    "    \n",
    "    # Fast version should complete in reasonable time\n",
    "    assert fast_time < 1.0, \"Optimized version is still too slow\"\n",
    "    \n",
    "    print(\"✓ Question 4 tests passed!\")\n",
    "\n",
    "test_question_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba75575-2cd5-4267-8c21-14b41d30d9ee",
   "metadata": {},
   "source": [
    "# ***Fix Function with Edge Cases***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4085a0da-79a0-457f-9536-30c1c9f8c15d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def calculate_stats(numbers):\n",
    "    \"\"\"\n",
    "    Calculate basic statistics for a list of numbers.\n",
    "    Handles edge cases: empty list, non-numeric values, division by zero.\n",
    "    \n",
    "    Args:\n",
    "        numbers: List of numbers (may contain invalid values)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Statistics including mean, median, mode, std_dev, count\n",
    "              Returns 'error' key if input is invalid or empty.\n",
    "    \"\"\"\n",
    "    # Filter only numeric values\n",
    "    numeric_numbers = [x for x in numbers if isinstance(x, (int, float))]\n",
    "    \n",
    "    if not numeric_numbers:\n",
    "        return {\n",
    "            'mean': None,\n",
    "            'median': None,\n",
    "            'mode': None,\n",
    "            'std_dev': None,\n",
    "            'count': 0,\n",
    "            'error': 'No valid numeric values provided'\n",
    "        }\n",
    "    \n",
    "    count = len(numeric_numbers)\n",
    "    \n",
    "    # Mean\n",
    "    mean = sum(numeric_numbers) / count\n",
    "    \n",
    "    # Median\n",
    "    sorted_nums = sorted(numeric_numbers)\n",
    "    n = count\n",
    "    if n % 2 == 0:\n",
    "        median = (sorted_nums[n//2 - 1] + sorted_nums[n//2]) / 2\n",
    "    else:\n",
    "        median = sorted_nums[n//2]\n",
    "    \n",
    "    # Mode\n",
    "    counts = Counter(numeric_numbers)\n",
    "    mode = counts.most_common(1)[0][0] if counts else None\n",
    "    \n",
    "    # Standard deviation\n",
    "    if count > 1:\n",
    "        variance = sum((x - mean) ** 2 for x in numeric_numbers) / count\n",
    "        std_dev = math.sqrt(variance)\n",
    "    else:\n",
    "        std_dev = 0.0\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'median': median,\n",
    "        'mode': mode,\n",
    "        'std_dev': std_dev,\n",
    "        'count': count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25b13e36-5cb0-4840-a1d0-3d62fb8f2832",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case 1: [1, 2, 3, 4, 5]\n",
      "  Result: {'mean': 3.0, 'median': 3, 'mode': 1, 'std_dev': 1.4142135623730951, 'count': 5}\n",
      "\n",
      "Test case 2: []\n",
      "  Result: {'mean': None, 'median': None, 'mode': None, 'std_dev': None, 'count': 0, 'error': 'No valid numeric values provided'}\n",
      "\n",
      "Test case 3: [1]\n",
      "  Result: {'mean': 1.0, 'median': 1, 'mode': 1, 'std_dev': 0.0, 'count': 1}\n",
      "\n",
      "Test case 4: [1, 1, 1]\n",
      "  Result: {'mean': 1.0, 'median': 1, 'mode': 1, 'std_dev': 0.0, 'count': 3}\n",
      "\n",
      "Test case 5: [1, 'invalid', 3]\n",
      "  Result: {'mean': 2.0, 'median': 2.0, 'mode': 1, 'std_dev': 1.0, 'count': 2}\n",
      "\n",
      "Test case 6: [1, 2, None, 4]\n",
      "  Result: {'mean': 2.3333333333333335, 'median': 2, 'mode': 1, 'std_dev': 1.247219128924647, 'count': 3}\n",
      "\n",
      "✓ Question 5 tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_question_5():\n",
    "    test_cases = [\n",
    "        [1, 2, 3, 4, 5],           # Normal case\n",
    "        [],                        # Empty list\n",
    "        [1],                       # Single item\n",
    "        [1, 1, 1],                 # All same\n",
    "        [1, 'invalid', 3],         # Mixed types\n",
    "        [1, 2, None, 4]            # None values\n",
    "    ]\n",
    "    \n",
    "    for i, case in enumerate(test_cases):\n",
    "        print(f\"Test case {i+1}: {case}\")\n",
    "        result = calculate_stats(case)\n",
    "        print(f\"  Result: {result}\\n\")\n",
    "\n",
    "    # Assertions\n",
    "    result = calculate_stats([1, 2, 3, 4, 5])\n",
    "    assert result['mean'] == 3.0\n",
    "    assert result['median'] == 3.0\n",
    "    assert result['count'] == 5\n",
    "    \n",
    "    result = calculate_stats([42])\n",
    "    assert result['mean'] == 42\n",
    "    assert result['median'] == 42\n",
    "    assert result['mode'] == 42\n",
    "    assert result['std_dev'] == 0.0\n",
    "    \n",
    "    result = calculate_stats([])\n",
    "    assert 'error' in result\n",
    "    \n",
    "    result = calculate_stats([1, 'invalid', 3])\n",
    "    assert result['count'] == 2\n",
    "    \n",
    "    result = calculate_stats([5, 5, 5, 5])\n",
    "    assert result['mean'] == 5\n",
    "    assert result['std_dev'] == 0.0\n",
    "\n",
    "    print(\"✓ Question 5 tests passed!\")\n",
    "\n",
    "test_question_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2f5ae-15fb-4558-bef8-d961af8cb48e",
   "metadata": {},
   "source": [
    "# ***Question 6: Complete Partial Implementation (Pandas/Data)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fe5e21a-ac72-4a64-be57-1526c2e57336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_sales_data(df, group_by_column):\n",
    "    \"\"\"\n",
    "    Analyze sales data by grouping and calculating statistics.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['product', 'category', 'sales', 'profit']\n",
    "        group_by_column: Column name to group by\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with aggregated statistics:\n",
    "        ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']\n",
    "        Indexed by group_by_column.\n",
    "    \"\"\"\n",
    "    # Handle edge cases: empty df or missing group_by_column\n",
    "    required_cols = ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']\n",
    "    if df.empty or group_by_column not in df.columns:\n",
    "        return pd.DataFrame(columns=required_cols)\n",
    "\n",
    "    # Replace missing sales/profit with 0\n",
    "    df_copy = df.copy()\n",
    "    df_copy['sales'] = df_copy['sales'].fillna(0)\n",
    "    df_copy['profit'] = df_copy['profit'].fillna(0)\n",
    "\n",
    "    # Group by the specified column\n",
    "    grouped = df_copy.groupby(group_by_column).agg(\n",
    "        sales_sum=('sales', 'sum'),\n",
    "        sales_mean=('sales', 'mean'),\n",
    "        profit_sum=('profit', 'sum'),\n",
    "        profit_mean=('profit', 'mean')\n",
    "    )\n",
    "\n",
    "    # Calculate profit margin, handle division by zero\n",
    "    grouped['profit_margin'] = grouped.apply(\n",
    "        lambda row: row['profit_sum'] / row['sales_sum'] if row['sales_sum'] != 0 else np.nan,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19110737-c285-4335-bf28-8c4a6d77805e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sales_sum  sales_mean  profit_sum  profit_mean  profit_margin\n",
      "product                                                               \n",
      "A            330.0       110.0          67    22.333333        0.20303\n",
      "B            200.0       100.0          90    45.000000        0.45000\n",
      "C            330.0       165.0          65    32.500000        0.19697\n"
     ]
    }
   ],
   "source": [
    "sample_data = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'C', 'A', 'B', 'C', 'A'],\n",
    "    'category': ['Electronics', 'Electronics', 'Clothing', 'Electronics', 'Electronics', 'Clothing', 'Electronics'],\n",
    "    'sales': [100, 200, 150, 120, np.nan, 180, 110],\n",
    "    'profit': [20, 50, 30, 25, 40, 35, 22]\n",
    "})\n",
    "\n",
    "result = analyze_sales_data(sample_data, 'product')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7147245-7d28-4bda-8f10-9bccdaa8a646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 6 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "def test_question_6():\n",
    "    # Create test data\n",
    "    test_data = pd.DataFrame({\n",
    "        'product': ['A', 'B', 'A', 'B', 'A'],\n",
    "        'category': ['Cat1', 'Cat2', 'Cat1', 'Cat2', 'Cat1'],\n",
    "        'sales': [100, 200, 150, 300, 50],\n",
    "        'profit': [20, 40, 30, 60, 10]\n",
    "    })\n",
    "    \n",
    "    # Test grouping by product\n",
    "    result = analyze_sales_data(test_data, 'product')\n",
    "    \n",
    "    # Check structure\n",
    "    assert isinstance(result, pd.DataFrame), \"Should return DataFrame\"\n",
    "    assert len(result) == 2, \"Should have 2 groups (A and B)\"\n",
    "    \n",
    "    # Check required columns exist\n",
    "    required_cols = ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']\n",
    "    for col in required_cols:\n",
    "        assert col in result.columns, f\"Missing column: {col}\"\n",
    "    \n",
    "    # Check calculations for product A\n",
    "    product_a = result.loc['A'] if 'A' in result.index else result[result.index == 'A'].iloc[0]\n",
    "    assert product_a['sales_sum'] == 300, \"Product A sales sum should be 300\"\n",
    "    assert product_a['profit_sum'] == 60, \"Product A profit sum should be 60\"\n",
    "    \n",
    "    print(\"✓ Question 6 tests passed!\")\n",
    "test_question_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02edc8-9d72-47bb-a438-fed64530fba3",
   "metadata": {},
   "source": [
    "# ***Question 7: Refactor Messy AI Code (Clean Code)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a3bed3f-a704-40f6-9b3b-37c7369befc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean result: {'adult': {'count': 2, 'emails': ['user1@test.com', 'user4@test.com'], 'avg_age': 32.5}, 'senior': {'count': 1, 'emails': ['user2@test.com'], 'avg_age': 70.0}}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "def process_data(data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Processes a list of user dictionaries and categorizes them based on age.\n",
    "    \n",
    "    Categories:\n",
    "        - 'young_adult': 18 <= age < 25\n",
    "        - 'adult': 25 <= age < 65\n",
    "        - 'senior': age >= 65\n",
    "    \n",
    "    Only considers active users with valid email addresses containing '@'.\n",
    "    \n",
    "    Returns a dictionary with categories as keys and stats as values:\n",
    "        - count: Number of users in category\n",
    "        - emails: List of user emails\n",
    "        - avg_age: Average age of users in category\n",
    "    \"\"\"\n",
    "    def categorize_age(age: int) -> str:\n",
    "        if age >= 65:\n",
    "            return 'senior'\n",
    "        elif age >= 25:\n",
    "            return 'adult'\n",
    "        elif age >= 18:\n",
    "            return 'young_adult'\n",
    "        else:\n",
    "            return 'underage'  # Optional, won't be counted\n",
    "    \n",
    "    result: Dict[str, Dict[str, Any]] = {}\n",
    "    \n",
    "    for item in data:\n",
    "        if item.get('type') != 'user':\n",
    "            continue\n",
    "        if not item.get('active', False):\n",
    "            continue\n",
    "        age = item.get('age')\n",
    "        email = item.get('email')\n",
    "        if not isinstance(age, (int, float)) or age < 18:\n",
    "            continue\n",
    "        if not isinstance(email, str) or '@' not in email:\n",
    "            continue\n",
    "        \n",
    "        category = categorize_age(age)\n",
    "        if category not in result:\n",
    "            result[category] = {'count': 0, 'emails': [], 'total_age': 0}\n",
    "        \n",
    "        result[category]['count'] += 1\n",
    "        result[category]['emails'].append(email)\n",
    "        result[category]['total_age'] += age\n",
    "    \n",
    "    # Compute average age\n",
    "    for cat in result:\n",
    "        result[cat]['avg_age'] = result[cat]['total_age'] / result[cat]['count']\n",
    "        del result[cat]['total_age']\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ==========================\n",
    "# Test Question 7\n",
    "# ==========================\n",
    "test_data = [\n",
    "    {'type': 'user', 'active': True, 'age': 25, 'email': 'user1@test.com'},\n",
    "    {'type': 'user', 'active': True, 'age': 70, 'email': 'user2@test.com'},\n",
    "    {'type': 'user', 'active': False, 'age': 30, 'email': 'user3@test.com'},\n",
    "    {'type': 'admin', 'active': True, 'age': 35, 'email': 'admin@test.com'},\n",
    "    {'type': 'user', 'active': True, 'age': 20, 'email': 'invalid-email'},\n",
    "    {'type': 'user', 'active': True, 'age': 40, 'email': 'user4@test.com'},\n",
    "]\n",
    "\n",
    "clean_result = process_user_data_clean(test_data)\n",
    "print(\"Clean result:\", clean_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c58fac81-9b7a-461d-a502-380e82109c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 7 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "def test_question_7():\n",
    "    test_data = [\n",
    "        {'type': 'user', 'active': True, 'age': 25, 'email': 'user1@test.com'},\n",
    "        {'type': 'user', 'active': True, 'age': 70, 'email': 'user2@test.com'},\n",
    "        {'type': 'user', 'active': False, 'age': 30, 'email': 'user3@test.com'},\n",
    "        {'type': 'user', 'active': True, 'age': 20, 'email': 'user4@test.com'},\n",
    "    ]\n",
    "    \n",
    "    original_result = process_data(test_data)\n",
    "    clean_result = process_user_data_clean(test_data)\n",
    "    \n",
    "    # Results should be functionally equivalent\n",
    "    assert set(original_result.keys()) == set(clean_result.keys()), \"Categories don't match\"\n",
    "    \n",
    "    for category in original_result:\n",
    "        assert original_result[category]['count'] == clean_result[category]['count'], f\"Count mismatch for {category}\"\n",
    "        assert abs(original_result[category]['avg_age'] - clean_result[category]['avg_age']) < 0.01, f\"Average age mismatch for {category}\"\n",
    "    \n",
    "    print(\"✓ Question 7 tests passed!\")\n",
    "\n",
    "test_question_7()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8ecb1-71a2-4b6b-b58a-1fc4a9c24af3",
   "metadata": {},
   "source": [
    "# ***Question 8: Debug Complex Logic (Algorithms)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "763ea9e4-3a74-407f-a533-d81b8b96d157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def binary_search_buggy(arr, target):\n",
    "    \"\"\"\n",
    "    Fixed binary search implementation.\n",
    "    \n",
    "    Args:\n",
    "        arr: Sorted list of integers\n",
    "        target: Value to search for\n",
    "        \n",
    "    Returns:\n",
    "        int: Index of target if found, -1 otherwise\n",
    "    \"\"\"\n",
    "    left = 0\n",
    "    right = len(arr) - 1  # Fix 1: right should be last index, not len(arr)\n",
    "    \n",
    "    while left <= right:  # Fix 2: use <= to include last element\n",
    "        mid = (left + right) // 2\n",
    "        \n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1  # Fix 3: move past mid\n",
    "        else:\n",
    "            right = mid - 1  # Fix 4: move before mid\n",
    "    \n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ab5031b-8e1d-4856-9c0c-5d270e0d0f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 7 in [1, 3, 5, 7, 9, 11]: 3\n",
      "Searching for 1 in [1, 3, 5, 7, 9, 11]: 0\n",
      "Searching for 11 in [1, 3, 5, 7, 9, 11]: 5\n",
      "Searching for 6 in [1, 3, 5, 7, 9, 11]: -1\n",
      "Searching for 5 in [5]: 0\n",
      "Searching for 3 in [5]: -1\n",
      "Searching for 5 in []: -1\n"
     ]
    }
   ],
   "source": [
    "test_arrays = [\n",
    "    ([1, 3, 5, 7, 9, 11], 7),    # index 3\n",
    "    ([1, 3, 5, 7, 9, 11], 1),    # index 0\n",
    "    ([1, 3, 5, 7, 9, 11], 11),   # index 5\n",
    "    ([1, 3, 5, 7, 9, 11], 6),    # -1\n",
    "    ([5], 5),                     # 0\n",
    "    ([5], 3),                     # -1\n",
    "    ([], 5),                      # -1\n",
    "]\n",
    "\n",
    "for arr, target in test_arrays:\n",
    "    print(f\"Searching for {target} in {arr}: {binary_search_buggy(arr, target)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e8eec28-5a65-4777-8113-996217a78303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 8 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "def test_question_8():\n",
    "    # Test cases with expected results\n",
    "    test_cases = [\n",
    "        ([1, 3, 5, 7, 9, 11], 7, 3),      # Found at index 3\n",
    "        ([1, 3, 5, 7, 9, 11], 1, 0),      # Found at index 0\n",
    "        ([1, 3, 5, 7, 9, 11], 11, 5),     # Found at index 5\n",
    "        ([1, 3, 5, 7, 9, 11], 6, -1),     # Not found\n",
    "        ([1, 3, 5, 7, 9, 11], 0, -1),     # Less than min\n",
    "        ([1, 3, 5, 7, 9, 11], 12, -1),    # Greater than max\n",
    "        ([5], 5, 0),                       # Single element found\n",
    "        ([5], 3, -1),                      # Single element not found\n",
    "        ([], 5, -1),                       # Empty array\n",
    "    ]\n",
    "    \n",
    "    for arr, target, expected in test_cases:\n",
    "        result = binary_search_buggy(arr, target)\n",
    "        assert result == expected, f\"Failed for {target} in {arr}: expected {expected}, got {result}\"\n",
    "    \n",
    "    # Test that it actually uses binary search (check performance)\n",
    "    large_array = list(range(0, 10000, 2))  # [0, 2, 4, 6, ..., 9998]\n",
    "    result = binary_search_buggy(large_array, 5000)\n",
    "    assert result == 2500, \"Should find 5000 at index 2500\"\n",
    "    \n",
    "    print(\"✓ Question 8 tests passed!\")\n",
    "\n",
    "test_question_8()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673dbf5b-91f1-46ca-b537-8b63e0b247a9",
   "metadata": {},
   "source": [
    "# ***Question 9: Add Missing Fonctionnality***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c79dccf-5881-4c4d-ba65-5ed7c3febad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any, Optional, Dict\n",
    "from collections import OrderedDict\n",
    "import threading\n",
    "\n",
    "class SimpleCache:\n",
    "    \"\"\"\n",
    "    Enhanced cache with TTL, LRU eviction, size limit, statistics, and management methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size: int = 100, default_ttl: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize cache with size limit and default TTL.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.default_ttl = default_ttl\n",
    "        self._data = OrderedDict()  # key -> (value, expire_time)\n",
    "        self._lock = threading.Lock()\n",
    "        \n",
    "        # Statistics\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.evictions = 0\n",
    "\n",
    "    def _current_time(self) -> float:\n",
    "        return time.time()\n",
    "\n",
    "    def _is_expired(self, key: str) -> bool:\n",
    "        value, expire_time = self._data.get(key, (None, None))\n",
    "        if expire_time is None:\n",
    "            return False\n",
    "        return self._current_time() >= expire_time\n",
    "\n",
    "    def _evict_lru(self, count: int = 1) -> int:\n",
    "        evicted = 0\n",
    "        while evicted < count and self._data:\n",
    "            key, _ = self._data.popitem(last=False)  # Remove least recently used\n",
    "            evicted += 1\n",
    "            self.evictions += 1\n",
    "        return evicted\n",
    "\n",
    "    def cleanup_expired(self) -> int:\n",
    "        removed = 0\n",
    "        keys_to_delete = []\n",
    "        now = self._current_time()\n",
    "        with self._lock:\n",
    "            for key, (_, expire_time) in self._data.items():\n",
    "                if expire_time is not None and now >= expire_time:\n",
    "                    keys_to_delete.append(key)\n",
    "            for key in keys_to_delete:\n",
    "                self._data.pop(key, None)\n",
    "                removed += 1\n",
    "        return removed\n",
    "\n",
    "    def get(self, key: str) -> Optional[Any]:\n",
    "        with self._lock:\n",
    "            if key not in self._data:\n",
    "                self.misses += 1\n",
    "                return None\n",
    "            if self._is_expired(key):\n",
    "                self._data.pop(key, None)\n",
    "                self.misses += 1\n",
    "                return None\n",
    "            # Move to end to mark as recently used\n",
    "            self._data.move_to_end(key)\n",
    "            self.hits += 1\n",
    "            return self._data[key][0]\n",
    "\n",
    "    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:\n",
    "        expire_time = None\n",
    "        if ttl is None:\n",
    "            ttl = self.default_ttl\n",
    "        if ttl is not None:\n",
    "            expire_time = self._current_time() + ttl\n",
    "        with self._lock:\n",
    "            if key in self._data:\n",
    "                # Remove first to re-insert at end\n",
    "                self._data.pop(key)\n",
    "            elif len(self._data) >= self.max_size:\n",
    "                self._evict_lru(1)\n",
    "            self._data[key] = (value, expire_time)\n",
    "            self._data.move_to_end(key)\n",
    "\n",
    "    def delete(self, key: str) -> bool:\n",
    "        with self._lock:\n",
    "            if key in self._data:\n",
    "                self._data.pop(key)\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        with self._lock:\n",
    "            self._data.clear()\n",
    "            self.hits = 0\n",
    "            self.misses = 0\n",
    "            self.evictions = 0\n",
    "\n",
    "    def size(self) -> int:\n",
    "        with self._lock:\n",
    "            return len(self._data)\n",
    "\n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        with self._lock:\n",
    "            return {\n",
    "                \"hits\": self.hits,\n",
    "                \"misses\": self.misses,\n",
    "                \"evictions\": self.evictions,\n",
    "                \"current_size\": len(self._data)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9262db7-e8f4-4a58-a402-0b019d1857b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing TTL ===\n",
      "Immediately after set: temp_value\n",
      "After TTL expired: None\n",
      "\n",
      "=== Testing Size Limits & LRU ===\n",
      "Cache size after adding 3 items: 3\n",
      "After adding 'd': a=1, b=None, c=3, d=4\n",
      "\n",
      "=== Testing Statistics ===\n",
      "Cache statistics: {'hits': 4, 'misses': 1, 'evictions': 1, 'current_size': 3}\n",
      "\n",
      "=== Testing Cleanup ===\n",
      "Expired items removed: 3\n"
     ]
    }
   ],
   "source": [
    "# Test your enhanced implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Test TTL functionality\n",
    "    cache = SimpleCache(max_size=3, default_ttl=1)  # 1 second TTL\n",
    "    \n",
    "    print(\"=== Testing TTL ===\")\n",
    "    cache.set(\"temp_key\", \"temp_value\")\n",
    "    print(f\"Immediately after set: {cache.get('temp_key')}\")\n",
    "    time.sleep(1.1)\n",
    "    print(f\"After TTL expired: {cache.get('temp_key')}\")\n",
    "    \n",
    "    print(\"\\n=== Testing Size Limits & LRU ===\")\n",
    "    cache.clear()\n",
    "    cache.set(\"a\", 1, ttl=None)  # No expiration\n",
    "    cache.set(\"b\", 2, ttl=None)\n",
    "    cache.set(\"c\", 3, ttl=None)\n",
    "    print(f\"Cache size after adding 3 items: {cache.size()}\")\n",
    "    \n",
    "    # Access 'a' to make it recently used\n",
    "    cache.get(\"a\")\n",
    "# Add 'd' which should evict 'b' (least recently used)\n",
    "    cache.set(\"d\", 4, ttl=None)\n",
    "    print(f\"After adding 'd': a={cache.get('a')}, b={cache.get('b')}, c={cache.get('c')}, d={cache.get('d')}\")\n",
    "    \n",
    "    print(\"\\n=== Testing Statistics ===\")\n",
    "    stats = cache.get_stats()\n",
    "    print(f\"Cache statistics: {stats}\")\n",
    "    \n",
    "    print(\"\\n=== Testing Cleanup ===\")\n",
    "    cache.set(\"expire_me\", \"value\", ttl=1)\n",
    "    time.sleep(1.1)\n",
    "    removed_count = cache.cleanup_expired()\n",
    "    print(f\"Expired items removed: {removed_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0777a8e4-a0de-4585-b339-25df36b2ff7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing enhanced cache implementation...\n",
      "✓ All Question 9 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell \n",
    "import time\n",
    "\n",
    "def test_question_9():\n",
    "    print(\"Testing enhanced cache implementation...\")\n",
    "    \n",
    "    # Test 1: Basic functionality\n",
    "    cache = SimpleCache(max_size=3, default_ttl=60)\n",
    "    \n",
    "    cache.set(\"key1\", \"value1\")\n",
    "    cache.set(\"key2\", \"value2\")\n",
    "    \n",
    "    assert cache.get(\"key1\") == \"value1\", \"Basic get/set failed\"\n",
    "    assert cache.get(\"key2\") == \"value2\", \"Basic get/set failed\"\n",
    "    assert cache.size() == 2, f\"Expected size 2, got {cache.size()}\"\n",
    "    \n",
    "    # Test 2: TTL expiration\n",
    "    cache.clear()\n",
    "    cache.set(\"ttl_key\", \"ttl_value\", ttl=1)  # 1 second TTL\n",
    "    assert cache.get(\"ttl_key\") == \"ttl_value\", \"TTL key should be accessible immediately\"\n",
    "    \n",
    "    time.sleep(1.1)  # Wait for expiration\n",
    "    assert cache.get(\"ttl_key\") is None, \"TTL key should be expired and return None\"\n",
    "    \n",
    "    # Test 3: Size limits and LRU eviction\n",
    "    cache.clear()\n",
    "    cache.set(\"a\", 1)\n",
    "    cache.set(\"b\", 2) \n",
    "    cache.set(\"c\", 3)  # Cache is now full (max_size=3)\n",
    "    \n",
    "    # Access 'a' to make it recently used\n",
    "    cache.get(\"a\")\n",
    "    \n",
    "    # Add 'd' which should evict 'b' (least recently used)\n",
    "    cache.set(\"d\", 4)\n",
    "    \n",
    "    assert cache.get(\"a\") == 1, \"Recently used 'a' should not be evicted\"\n",
    "    assert cache.get(\"b\") is None, \"Least recently used 'b' should be evicted\"\n",
    "    assert cache.get(\"c\") == 3, \"'c' should still be in cache\"\n",
    "    assert cache.get(\"d\") == 4, \"Newly added 'd' should be in cache\"\n",
    "    assert cache.size() == 3, \"Cache size should remain at max_size\"\n",
    "    \n",
    "    # Test 4: Statistics tracking\n",
    "    cache.clear()\n",
    "    cache.set(\"stat_key\", \"stat_value\")\n",
    "    cache.get(\"stat_key\")  # Hit\n",
    "    cache.get(\"nonexistent\")  # Miss\n",
    "    \n",
    "    stats = cache.get_stats()\n",
    "    required_stats = [\"hits\", \"misses\", \"evictions\", \"current_size\"]\n",
    "    for stat in required_stats:\n",
    "        assert stat in stats, f\"Missing statistic: {stat}\"\n",
    "    \n",
    "    assert stats[\"hits\"] > 0, \"Should have recorded hits\"\n",
    "    assert stats[\"misses\"] > 0, \"Should have recorded misses\"\n",
    "    assert stats[\"current_size\"] == 1, \"Should track current size\"\n",
    "\n",
    "    # Test 5: Manual cleanup\n",
    "    cache.clear()\n",
    "    cache.set(\"expire1\", \"value1\", ttl=1)\n",
    "    cache.set(\"expire2\", \"value2\", ttl=1)\n",
    "    cache.set(\"keep\", \"value3\", ttl=None)  # No expiration\n",
    "    \n",
    "    time.sleep(1.1)  # Wait for expiration\n",
    "    removed_count = cache.cleanup_expired()\n",
    "    \n",
    "    assert removed_count == 2, f\"Should have removed 2 expired items, removed {removed_count}\"\n",
    "    assert cache.get(\"keep\") == \"value3\", \"Non-expiring item should remain\"\n",
    "    assert cache.size() == 1, \"Only one item should remain after cleanup\"\n",
    "    \n",
    "    # Test 6: Edge cases\n",
    "    cache.clear()\n",
    "    assert cache.size() == 0, \"Cache should be empty after clear\"\n",
    "    assert cache.get(\"nonexistent\") is None, \"Getting non-existent key should return None\"\n",
    "    assert cache.delete(\"nonexistent\") == False, \"Deleting non-existent key should return False\"\n",
    "    \n",
    "    # Test delete functionality\n",
    "    cache.set(\"delete_me\", \"value\")\n",
    "    assert cache.delete(\"delete_me\") == True, \"Deleting existing key should return True\"\n",
    "    assert cache.get(\"delete_me\") is None, \"Deleted key should not be accessible\"\n",
    "    \n",
    "    print(\"✓ All Question 9 tests passed!\")\n",
    "\n",
    "test_question_9()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5766c5-147d-40b8-97c4-46804f00c40e",
   "metadata": {},
   "source": [
    "# ***Question 10: Integration Challenge (Multiple Components)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74bc2399-f6fb-4a5a-aebf-555fb8b68f44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "           ANALYSIS REPORT\n",
      "==================================================\n",
      "\n",
      "Section 1: Analyzed 3 items (3 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 30.00\n",
      "    max_value: 40\n",
      "    min_value: 20\n",
      "    total_value: 90\n",
      "    success_rate: 1.00\n",
      "\n",
      "Section 2: Analyzed 2 items (2 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 30.00\n",
      "    max_value: 50\n",
      "    min_value: 10\n",
      "    total_value: 60\n",
      "    success_rate: 1.00\n",
      "\n",
      "Section 3: Analyzed 2 items (2 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 70.00\n",
      "    max_value: 80\n",
      "    min_value: 60\n",
      "    total_value: 140\n",
      "    success_rate: 1.00\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "\n",
    "# --- Component 1: DataProcessor ---\n",
    "class DataProcessor:\n",
    "    \"\"\"AI Component 1 - processes raw data and returns structured dict\"\"\"\n",
    "    def process_data(self, raw_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        if not isinstance(raw_data, list):\n",
    "            raise ValueError(\"Expected list input\")\n",
    "        result = {\n",
    "            'total_items': len(raw_data),\n",
    "            'processed_items': [],\n",
    "            'metadata': {'processing_time': 0.1, 'timestamp': '2024-01-01T12:00:00Z'}\n",
    "        }\n",
    "        for item in raw_data:\n",
    "            if isinstance(item, dict) and 'value' in item:\n",
    "                result['processed_items'].append({\n",
    "                    'id': item.get('id', 'unknown'),\n",
    "                    'processed_value': item['value'] * 2,\n",
    "                    'original_value': item['value'],\n",
    "                    'status': 'processed'\n",
    "                })\n",
    "            else:\n",
    "                result['processed_items'].append({\n",
    "                    'id': 'error',\n",
    "                    'processed_value': 0,\n",
    "                    'original_value': None,\n",
    "                    'status': 'failed'\n",
    "                })\n",
    "        return result\n",
    "\n",
    "# --- Component 2: AnalyticsEngine ---\n",
    "class AnalyticsEngine:\n",
    "    \"\"\"AI Component 2 - performs analytics on data, expects JSON string input\"\"\"\n",
    "    def analyze(self, json_data_string: str) -> Tuple[Optional[str], Union[Dict[str, float], str]]:\n",
    "        try:\n",
    "            data = json.loads(json_data_string)\n",
    "        except json.JSONDecodeError:\n",
    "            return None, \"Invalid JSON format\"\n",
    "        if not isinstance(data, dict) or 'processed_items' not in data:\n",
    "            return None, \"Missing processed_items in data structure\"\n",
    "        items = data['processed_items']\n",
    "        if not isinstance(items, list):\n",
    "            return None, \"processed_items must be a list\"\n",
    "        values = []\n",
    "        failed_count = 0\n",
    "        for item in items:\n",
    "            if isinstance(item, dict) and item.get('status') == 'processed':\n",
    "                if 'processed_value' in item and isinstance(item['processed_value'], (int, float)):\n",
    "                    values.append(item['processed_value'])\n",
    "            else:\n",
    "                failed_count += 1\n",
    "        if not values:\n",
    "            return None, \"No valid numeric data found for analysis\"\n",
    "        summary = f\"Analyzed {len(items)} items ({len(values)} successful, {failed_count} failed)\"\n",
    "        metrics = {\n",
    "            'avg_value': sum(values) / len(values),\n",
    "            'max_value': max(values),\n",
    "            'min_value': min(values),\n",
    "            'total_value': sum(values),\n",
    "            'success_rate': len(values) / len(items) if items else 0.0\n",
    "        }\n",
    "        return summary, metrics\n",
    "\n",
    "# --- Component 3: ReportGenerator ---\n",
    "class ReportGenerator:\n",
    "    \"\"\"AI Component 3 - generates reports from analytics results\"\"\"\n",
    "    def generate_report(self, analytics_results_list: List[Tuple[Optional[str], Union[Dict, str]]]) -> str:\n",
    "        if not isinstance(analytics_results_list, list):\n",
    "            return \"Error: Expected list input for report generation\"\n",
    "        if not analytics_results_list:\n",
    "            return \"Error: No data provided for report generation\"\n",
    "        report_lines = [\n",
    "            \"=\" * 50,\n",
    "            \"           ANALYSIS REPORT\",\n",
    "            \"=\" * 50\n",
    "        ]\n",
    "        for i, result in enumerate(analytics_results_list):\n",
    "            if not isinstance(result, tuple) or len(result) != 2:\n",
    "                report_lines.append(f\"\\nSection {i+1}: Invalid data format - expected (summary, metrics) tuple\")\n",
    "                continue\n",
    "            summary, metrics = result\n",
    "            if summary is None:\n",
    "                report_lines.append(f\"\\nSection {i+1}: Analysis failed\")\n",
    "                report_lines.append(f\"  Error: {metrics}\")\n",
    "                continue\n",
    "            report_lines.append(f\"\\nSection {i+1}: {summary}\")\n",
    "            if isinstance(metrics, dict):\n",
    "                report_lines.append(\"  Metrics:\")\n",
    "                for key, value in metrics.items():\n",
    "                    if isinstance(value, float):\n",
    "                        report_lines.append(f\"    {key}: {value:.2f}\")\n",
    "                    else:\n",
    "                        report_lines.append(f\"    {key}: {value}\")\n",
    "            else:\n",
    "                report_lines.append(f\"  Metrics: {metrics}\")\n",
    "        report_lines.append(\"\\n\" + \"=\" * 50)\n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "# --- Adapter / Helper Functions ---\n",
    "def dict_to_json_adapter(data_dict: Dict[str, Any]) -> str:\n",
    "    try:\n",
    "        return json.dumps(data_dict)\n",
    "    except (TypeError, ValueError):\n",
    "        return json.dumps({'processed_items': []})\n",
    "\n",
    "def validate_and_clean_raw_data(raw_data: Any) -> List[Dict[str, Any]]:\n",
    "    cleaned = []\n",
    "    if not isinstance(raw_data, list):\n",
    "        return cleaned\n",
    "    for item in raw_data:\n",
    "        if isinstance(item, dict) and 'value' in item:\n",
    "            val = item['value']\n",
    "            if isinstance(val, (int, float)):\n",
    "                cleaned.append({'id': item.get('id', 'unknown'), 'value': val})\n",
    "    return cleaned\n",
    "\n",
    "# --- Integrated Pipeline ---\n",
    "def integrated_pipeline(raw_data_list: List[Any]) -> str:\n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    analytics_results = []\n",
    "    if not raw_data_list:\n",
    "        return reporter.generate_report([])\n",
    "    for raw_data in raw_data_list:\n",
    "        try:\n",
    "            cleaned_data = validate_and_clean_raw_data(raw_data)\n",
    "            processed_dict = processor.process_data(cleaned_data)\n",
    "            json_data = dict_to_json_adapter(processed_dict)\n",
    "            result = analytics.analyze(json_data)\n",
    "            analytics_results.append(result)\n",
    "        except Exception as e:\n",
    "            analytics_results.append((None, f\"Unexpected error: {e}\"))\n",
    "    return reporter.generate_report(analytics_results)\n",
    "\n",
    "# --- Sample Data Creation ---\n",
    "def create_sample_data() -> List[List[Dict[str, Any]]]:\n",
    "    return [\n",
    "        [{'id': 'A1', 'value': 10}, {'id': 'A2', 'value': 20}, {'id': 'A3', 'value': 15}],\n",
    "        [{'id': 'B1', 'value': 5}, {'id': 'B2', 'value': 25}],\n",
    "        [{'id': 'C1', 'value': 30}, {'id': 'C2'}, {'value': 40}, {'id': 'C4', 'value': 'invalid'}]\n",
    "    ]\n",
    "\n",
    "# --- Test Run ---\n",
    "if __name__ == \"__main__\":\n",
    "    sample_datasets = create_sample_data()\n",
    "    report = integrated_pipeline(sample_datasets)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4171d385-ebf7-4c73-b22f-762e11b3b7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing component integration...\n",
      "\n",
      "=== Testing Individual Components ===\n",
      "DataProcessor output: {'total_items': 1, 'processed_items': [{'id': 'test', 'processed_value': 20, 'original_value': 10, 'status': 'processed'}], 'metadata': {'processing_time': 0.1, 'timestamp': '2024-01-01T12:00:00Z'}}\n",
      "AnalyticsEngine output: ('Analyzed 1 items (1 successful, 0 failed)', {'avg_value': 20.0, 'max_value': 20, 'min_value': 20, 'total_value': 20, 'success_rate': 1.0})\n",
      "ReportGenerator output:\n",
      "==================================================\n",
      "           ANALYSIS REPORT\n",
      "==================================================\n",
      "\n",
      "Section 1: Analyzed 1 items (1 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 20.00\n",
      "    max_value: 20\n",
      "    min_value: 20\n",
      "    total_value: 20\n",
      "    success_rate: 1.00\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Testing Integrated Pipeline ===\n",
      "Integration successful!\n",
      "==================================================\n",
      "           ANALYSIS REPORT\n",
      "==================================================\n",
      "\n",
      "Section 1: Analyzed 3 items (3 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 30.00\n",
      "    max_value: 40\n",
      "    min_value: 20\n",
      "    total_value: 90\n",
      "    success_rate: 1.00\n",
      "\n",
      "Section 2: Analyzed 2 items (2 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 30.00\n",
      "    max_value: 50\n",
      "    min_value: 10\n",
      "    total_value: 60\n",
      "    success_rate: 1.00\n",
      "\n",
      "Section 3: Analyzed 2 items (2 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 70.00\n",
      "    max_value: 80\n",
      "    min_value: 60\n",
      "    total_value: 140\n",
      "    success_rate: 1.00\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing component integration...\")\n",
    "    \n",
    "    # Test individual components first\n",
    "    print(\"\\n=== Testing Individual Components ===\")\n",
    "    \n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    \n",
    "    # Test DataProcessor\n",
    "    test_data = [{'id': 'test', 'value': 10}]\n",
    "    processed = processor.process_data(test_data)\n",
    "    print(f\"DataProcessor output: {processed}\")\n",
    "    \n",
    "    # Test AnalyticsEngine\n",
    "    json_data = json.dumps(processed)\n",
    "    analysis_result = analytics.analyze(json_data)\n",
    "    print(f\"AnalyticsEngine output: {analysis_result}\")\n",
    "    \n",
    "    # Test ReportGenerator\n",
    "    report = reporter.generate_report([analysis_result])\n",
    "    print(f\"ReportGenerator output:\\n{report}\")\n",
    "    \n",
    "    print(\"\\n=== Testing Integrated Pipeline ===\")\n",
    "    \n",
    "    # Test full pipeline\n",
    "    sample_datasets = create_sample_data()\n",
    "    \n",
    "    try:\n",
    "        final_report = integrated_pipeline(sample_datasets)\n",
    "        print(\"Integration successful!\")\n",
    "        print(final_report)\n",
    "    except Exception as e:\n",
    "        print(f\"Integration failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b7eec22-e86a-454e-bd5d-d4f7a633072d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing integrated pipeline...\n",
      "✓ All Question 10 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "def test_question_10():\n",
    "    print(\"Testing integrated pipeline...\")\n",
    "    \n",
    "    # Test 1: Individual component functionality\n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    \n",
    "    # Test DataProcessor\n",
    "    test_data = [{'id': 'test1', 'value': 10}, {'id': 'test2', 'value': 20}]\n",
    "    processed = processor.process_data(test_data)\n",
    "    \n",
    "    assert isinstance(processed, dict), \"DataProcessor should return dict\"\n",
    "    assert 'total_items' in processed, \"Missing total_items in processed data\"\n",
    "    assert 'processed_items' in processed, \"Missing processed_items in processed data\"\n",
    "    assert processed['total_items'] == 2, \"Should count items correctly\"\n",
    "    \n",
    "    # Test AnalyticsEngine\n",
    "    json_data = json.dumps(processed)\n",
    "    summary, metrics = analytics.analyze(json_data)\n",
    "    \n",
    "    assert summary is not None, \"Analytics should return valid summary\"\n",
    "    assert isinstance(metrics, dict), \"Analytics should return metrics dict\"\n",
    "    assert 'avg_value' in metrics, \"Missing avg_value in metrics\"\n",
    "    \n",
    "    # Test ReportGenerator\n",
    "    report = reporter.generate_report([(summary, metrics)])\n",
    "    \n",
    "    assert isinstance(report, str), \"Report should be string\"\n",
    "    assert \"ANALYSIS REPORT\" in report, \"Report should contain header\"\n",
    "    assert \"Section 1\" in report, \"Report should contain section\"\n",
    "    \n",
    "    # Test 2: Data validation and cleaning\n",
    "    cleaned_data = validate_and_clean_raw_data([\n",
    "        {'id': 'valid', 'value': 10},\n",
    "        {'value': 20},  # Missing id\n",
    "        {'id': 'invalid'},  # Missing value\n",
    "        'invalid_format'  # Wrong format\n",
    "    ])\n",
    "    \n",
    "    assert isinstance(cleaned_data, list), \"Should return list\"\n",
    "    # Should handle invalid data gracefully\n",
    "    \n",
    "    # Test 3: Integration adapters\n",
    "    test_dict = {'processed_items': [{'processed_value': 10}]}\n",
    "    json_str = dict_to_json_adapter(test_dict)\n",
    "    \n",
    "    assert isinstance(json_str, str), \"Should return JSON string\"\n",
    "    # Should be valid JSON\n",
    "    parsed = json.loads(json_str)\n",
    "    assert parsed == test_dict, \"Should preserve data structure\"\n",
    "    \n",
    "    # Test 4: Full pipeline integration\n",
    "    sample_datasets = [\n",
    "        [{'id': 'A1', 'value': 10}, {'id': 'A2', 'value': 20}],\n",
    "        [{'id': 'B1', 'value': 5}],\n",
    "        []  # Empty dataset\n",
    "    ]\n",
    "    \n",
    "    final_report = integrated_pipeline(sample_datasets)\n",
    "    \n",
    "    assert isinstance(final_report, str), \"Pipeline should return string report\"\n",
    "    assert \"ANALYSIS REPORT\" in final_report, \"Should contain report header\"\n",
    "    \n",
    "    # Should handle multiple sections\n",
    "    assert \"Section 1\" in final_report, \"Should have first section\"\n",
    "    assert \"Section 2\" in final_report, \"Should have second section\"\n",
    "    \n",
    "    # Test 5: Error handling\n",
    "    # Test with invalid input\n",
    "    error_report = integrated_pipeline([])\n",
    "    assert isinstance(error_report, str), \"Should handle empty input gracefully\"\n",
    "    \n",
    "    # Test with malformed data\n",
    "    malformed_report = integrated_pipeline([[\"not\", \"a\", \"dict\", \"list\"]])\n",
    "    assert isinstance(malformed_report, str), \"Should handle malformed data\"\n",
    "    \n",
    "    # Test 6: Edge cases\n",
    "    edge_cases = [\n",
    "        [{'id': 'only_id'}],  # Missing value\n",
    "        [{'value': 42}],      # Missing id\n",
    "        [{}],                 # Empty dict\n",
    "    ]\n",
    "    \n",
    "    edge_report = integrated_pipeline(edge_cases)\n",
    "    assert isinstance(edge_report, str), \"Should handle edge cases\"\n",
    "    assert \"ANALYSIS REPORT\" in edge_report, \"Should still generate report structure\"\n",
    "    \n",
    "    print(\"✓ All Question 10 tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_question_10()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
